{"meta":{"title":"Jang. Inspiration","subtitle":"Jangyeong's Inspiration Note","description":"blog","author":"Jangyeong Kim","url":"https://longshiine.github.io","root":"/"},"pages":[{"title":"About","date":"2022-12-28T08:42:03.929Z","updated":"2022-12-04T10:12:48.000Z","comments":true,"path":"about/index.html","permalink":"https://longshiine.github.io/about/index.html","excerpt":"","text":"Hi! 😆"}],"posts":[{"title":"2022년 회고, 스타트업 실패에 대하여","slug":"2022/12/31/2022년-회고","date":"2022-12-31T09:32:06.000Z","updated":"2023-01-06T06:52:04.621Z","comments":true,"path":"2022/12/31/2022년-회고/","link":"","permalink":"https://longshiine.github.io/2022/12/31/2022%EB%85%84-%ED%9A%8C%EA%B3%A0/","excerpt":"","text":"잠시 멈춰두었던 시계를 다시 돌릴 시간이다.22년을 보내고, 23년을 맞이하기 위해 노트를 펼쳤다. 불안, 긴장, 책임감, 무력함. 한 해동안 나를 휘감고 있었던 감정들을 내려두고 숨을 가다듬는 시간을 보내다보니, 정신이 맑아지며 그간 정리하지 못했던 생각들이 정리되기 시작했다. ‘멈추면, 비로소 보이는 것들’은 분명 존재한다. 왜 실패했는가?첫째는 창업에 있어 스스로의 뚜렷한 목표가 부재했다. 커다란 부를 이루고 싶다던지, 특정 고객의 문제를 반드시 풀어내고야 말겠다던지 따위의 개인적인 목표가 부재했다. 목표의 부재는 방향성의 모호함으로, 방황의 연속으로 이어졌다. 어릴적부터 창업을 하고 싶었고, Generative AI는 혁신적이었으며, 좋은 기회가 주어져 시작되었던 첫 창업은 뚜렷한 개인적 목표와 동작하는 비즈니스 사이의 공생 관계를 만들어내지 못하며 한 편의 표류기가 되고 말았다. 둘째는 남들과 비교하며, 조급해했다. 비교대상은 주로, 성공한 실리콘밸리 스타트업의 초기 모습 혹은 현재 활동 중인 동년배의 한국 창업가들이었다. 뚜렷하고 내적인 목표가 부재했기에, 외적 기준에 따른 비교가 자주 일어났다. 그러나 외적 비교를 통한 진단이나 개선 방안을 마련하는 행위는 자신만의 비즈니스를 일구는 데에 크게 도움이 되지 않았다. 오히려 마음이 조급해지고 심리적으로 위축될 뿐이였다. 더구나 누군가와 함께 하는 상황, 나아가 책임을 져야하는 자리에 있는 상황에서의 부담감은 꽤나 커다란 그릇을 요했고, 나의 종지만한 그릇에 담아내고자 할 때면 늘 넘쳐버리고야 말았다. 셋째는 능력과 경험이 부족했다. 이 부분의 경우에는 같은 조건에서도 충분히 훌륭한 기업을 일구어내는 사례들이 정말 많기에, 포기하지 않는 시간이 이어지다보면 극복되는 요인이라 생각한다. 때문에 내가 끝까지 붙잡고 늘어지지 못한 탓이 크다고 생각한다. 물론 나라는 사람이 무지한 상태에서 홀로 배우며, 기업을 성장시키는 재능을 갖추지 못하고 있는 탓일 수도 있다. 그러나, 그러한 재능을 갖추고 있는지 여부는 자신이 직접 몸을 던져보는 수 밖에 없기에, 배움의 의미가 더 컸다고 생각한다. 능력과 경험을 갖추는 시간이 필요하겠다. 무엇을 하고 싶은가?여전히 스타트업이 하고 싶다. 다만, 앞선 실패로 새로운 창업 이전 메워야 할 부족함들을 파악할 수 있었다. 뚜렷한 목표를 갖추어 내적 동기를 강력한 성장 엔진으로 삼아야 할 것이며, 필요한 능력과 경험을 갖추어야 할 것이다. 짧지 않은 시간일 것이다. 솔직히 현재의 내게는 커다란 부를 갖추어 마음껏 소비하고 싶다는 욕망이 크게 없다. 부를 이루게 된다고 해도 또다시 새로운 창업을 하고 싶을 것만 같다. 돈의 역할은 성과지표 혹은 임팩트를 측정하는 수단인 듯 하다. 임팩트를 만들어내는 창업가들이 존경스럽다. 사람들의 삶에 영향을 미치는 제품들이 경이롭다. 훌륭한 제품, 인재, 문화의 3요소를 갖추고 있는 기업들이 멋지다. 오랜기간 혁신을 만들어내는 기업을 만들고 싶다. 높은 기업가치는 분명 사회에 긍정적 영향을 돌려주는 데에 더 큰 힘을 낼 수 있을 것이므로, 가능하다면 큰 기업을. 무모한 생각이나, 목표는 항상 크게 잡는 것이 좋다. 그래야 질리지 않고 오래간다. 현재로서는 AI가 내게 일련의 본진이 되어줄 것 같고, 관련된 공부나 경험을 이어나갈 것 같다. AI가 삶의 방식을 바꾸어 나갈 것임을 믿어 의심치 않는다. 스마트폰을 통한 초연결 이후 새로운 패러다임은 AI와 로봇을 통한 초자동화일 것이다. 다만 실패를 통해 배운 점 중 또다른 하나는 미래를 보다 구체적으로 그릴 필요가 있다는 점이다. 추상적이고 단편적인 토대 위에 세워진 전략은 금방 무너진다. 패러다임에서 특정 문제로, 기술로, 제품으로, 전략으로, 결과로 명료하게 치환해 낼 수 있어야 할 것이다. 너무나 귀중한 시간이었다.도전의 기댓값은 실패이고, 과정에서 배우고 얻을 수 있는 것은 수없이 많다. 다만, 실패는 훈장이 아니다. 철저한 반추의 대상이며 같은 원인이 되풀이 되어선 안된다. 기회를 준 분들, 응원을 보내준 사람들을 생각하면 ‘좋은 경험이었다’ 퉁칠 수 없는 노릇이다. 무모한 도전과 실패에도 격려의 메세지를 보내준 많은 이들에게 부끄럽지 않은 삶을 살아내는 한 해가 되길 바라며. 굿바이 2022, 굿바이 비디에이아이.","categories":[{"name":"Writing","slug":"Writing","permalink":"https://longshiine.github.io/categories/Writing/"}],"tags":[{"name":"스타트업","slug":"스타트업","permalink":"https://longshiine.github.io/tags/%EC%8A%A4%ED%83%80%ED%8A%B8%EC%97%85/"},{"name":"회고","slug":"회고","permalink":"https://longshiine.github.io/tags/%ED%9A%8C%EA%B3%A0/"}]},{"title":"<히말라야 환상방황> - EP1. 혼돈의 타멜거리","slug":"2022/12/28/히말라야-환상방황-1","date":"2022-12-28T09:14:19.000Z","updated":"2023-01-06T06:29:10.531Z","comments":true,"path":"2022/12/28/히말라야-환상방황-1/","link":"","permalink":"https://longshiine.github.io/2022/12/28/%ED%9E%88%EB%A7%90%EB%9D%BC%EC%95%BC-%ED%99%98%EC%83%81%EB%B0%A9%ED%99%A9-1/","excerpt":"","text":"정유정 작가님의 에세이와는 목적지만 같을 뿐 하나도 비슷한 구석이 없습니다..ㅎ 제멋대로, 엉망진창으로 22년 버전의 환상방황을 담아보았습니다. 내부에너지가 고갈되고 무기력 해졌을 때, 다시 세상에 맞설 용기를 얻기 위해 그녀가 선택한 여행이기도, 또한 제가 선택한 새로운 도전이기도 합니다. 가볍고 재밌게 읽힌다면 좋겠네요 :) EP 1. 혼돈의 타멜거리네팔행 비행기에 올랐다.감상이 어떻냐고? 설렘 반 두려움 반이 크다. 해외에서 잊혀진 이방인으로서의 감각을 느끼는 것이 설레고, 헤쳐나가야 할 미지의 것들이 두렵다. 공항노숙 부터 타멜거리를 찾는 것. 포카라행 야간버스를 예매하는 일부터 포카라에 도착해 윈드폴까지 이동하는 일까지. 또한 앞으로 무얼하고 살아야 하는지에 대한 고민을 해야한다는 사실까지. 나는 무얼하고 싶지? 감히 머리로 정리해 낼 수나 있는 질문일까. 디테일이 아닌 커다란 방향성, 동서남북 정도의 거시적인 방향성 정도만 수립해보자 다짐하며, 비행기에 올랐다. 경험의 폭이 그 사람의 선택지 혹은 한계를 늘려줄 수 있을지 모른다. 그렇기에 해보고 싶은게 있다면 주저하지 말고 해봐야 한다. 무척이나 용기가 필요한 일이다. 남들의 시선도 의식하게 되고(사실 그들은 내게 별 관심도 없을테지만), 두려움도 엄습해온다. 그러나 해봐야 한다. 그래야 후회가 남지 않는다. 문제는 그 끝에 답을 찾을 수 있을 것인가 하는 점이겠지만. 올 한해 창업에 도전했던 것에 대해선 전혀 후회하지 않는다. 아쉬움이 남지도 않는다. 다만, 결과를 내지 못해서 부끄럽고, 앞으로가 불안할 따름이다. 웃긴 것은 최선을 다했고 나는 그 이상 할 수 없었음에도, 부끄러움이라는 감정이 남았다는 것이다. 왜 일까, 감정을 찬찬히 뜯어보았다. 약간의 오만과 죄책감이 뒤섞여 있었다. 처음부터 잘할 수 없고, 모자랄 수 밖에 없다는 사실을 인정하지 않으려는 오만, 그리고 큰 투자금을 날려먹은 것에 대한 죄책감. 크게 두 가지 감정이 섞여 부끄러움이라는 새로운 감정으로 나를 괴롭히고 있었다. 남아있던 감정의 정체를 이해하긴 했으나, 어떻게 처리할 수 있을지 아직은 잘 모르겠다. 조금은 힌트를 얻을 수 있을까, 한국에서 들고 온 서머싯 몸의 소설 한 권을 펼쳐 읽기 시작한다. 책을 읽다보니 어느새 인도 뉴델리 공항에 도착했다. 방황하며 걷기를 1시간여, 드문드문 한국인들의 눈치를 슬쩍씩 봐가며 공항에서의 14시간을 보내는 중이다. 내일의 일정이 살짝 걱정된다. 내일은 카트만두 공항에 내려 네팔에 입국하고 타멜거리로 이동, 포카라로 갈 수 있는 방법을 알아내야 한다. 크게 두 가지의 퀘스트다. 1. 타멜거리로 이동하기 2. 포카라로 이동하기 철저한 이방인으로서 잘 생존해낼 수 있을까. 또 여행이 끝나면 내가 품고왔던 물음에 답할 수 있을까? 지금 맞은 편에는 한국 분으로 추정되는 분이 계신데, 말을 걸지 말지도 고민된다. ‘어디로 가시냐’, ‘카트만두에 가시냐’, ‘이후 일정은 어떻게 되시냐’, 뭐 이런 얘기를 해야하나. 무슨 이유 때문인지는 모르겠지만 전반적으로 멍하다. 지금 무슨 이야기를 하고 있는지도 모르겠다. 잠이 조금 필요한 것일지도. 맞은편 분도 어지간히 심심하시겠지. 14시간이라니. 결국 말 거는 일은 포기하고, 8번 게이트 옆 라꾸라꾸에서 잠을 청해본다. 타멜 거리에서 커피를 한잔 마시는 중이다.네팔 트리부반 공항에 무사히 도착한 뒤, 비자를 발급받는 중 한국인 분들께 말을 걸었다. 인도에서 네팔로 향했던 에어인디아 비행기에 한국인은 총 (나 포함) 8명이 있었다. 커플 2쌍과 봉사활동을 온 2명, 그리고 나처럼 홀로 온 한 분. 다들 목적지를 향해 잘 출발했고, 포카라로 무턱대고 향하는 사람은 홀로 온 두 사람 뿐이었다. 그 분께서 비행기를 타고 간다고 하시기에 가격이나 알아볼 요량으로 따라나섰고, 길을 헤매다 겨우 찾아 들어간 네팔 국내선 공항에서 부르는 포카라행 가격은 120$. 가격도 가격이고 타멜거리를 한번 보고 싶은 생각이 컸기에 깔끔하게 버스를 타고 가기로 결심했다. 택시를 타려고 이동하던 중, 한 명이 말을 걸어온다. 타멜로 가는지 물어본다. 흥정의 시간이 왔음을 직감했다. 얼마냐 물었더니 800 루피라고 한다(1루피에 10원 꼴이니, 루피x10 으로 계산을 하면 한국식으로 계산하기 편하다). 내가 가고 싶은 가격은 650 루피 정도, 그래서 500 루피를 불렀다. 그랬더니 노발대발하며 저기 Pre-Paid 택시는 1,000 루피를 받는다며, 800 루피 아니면 안 간단다. 그래 그럼 말아ㅋㅋ 하고 나는 내 갈길 가려는데 계속 쫓아오며, 얼마를 원하냐 자꾸 물어본다. 500 루피라 말하지 않았느냐, 그랬더니 아니 그건 좀 오바란다. 그래서 600 루피까지 괜찮겠다 말하니, 650 루피에 가잔다. 그래 이거지. 선심쓰는척 ‘Okay 650’ 를 말하고, 택시에 올랐다. 택시를 타는 내내 650 루피에 가기로 이야기 했다는 점을 계속 상기시켰다. 또 도착해서 딴소리 할까 경계하기 위함이었다. 다행히도, 650 루피에 안전히 타멜거리에 도착할 수 있었다. 첫번째 퀘스트를 잘 클리어 해냈음이 나름 뿌듯하기도 했다. 타멜거리는 네팔의 수도 카트만두에서도 가장 유명한 거리이다. 히말라야 관광이 전체 소득에서 많은 비율을 차지하는 만큼, 타멜거리 곳곳에는 트레킹 샵, 트레킹 여행사 등이 즐비하다. 나는 타멜거리 구경도 구경이나, 포카라행 야간버스의 예매가 필요했기에 한국인들을 주로 대상으로 한다는 제이빌 여행사를 찾아가 보았다. 사실 여행사가 야간버스 티켓까지 취급하는지는 몰랐고 그냥 예매 위치나 알아볼까하여 무턱대고 찾아간 것인데, 이게 웬걸. 야간버스 티켓을 15$에 바로 끊을 수 있었다. 오늘은 모든 일정이 생각보다 술술 잘풀린다. 걱정해두길 잘한건가. 예매 이후에는 모모스타라는 로컬 맛집에 들렀다. 첫 네팔 음식을 시도하는 것이었는데, 무난하게 미리 찾아두었던 버팔로 땜뚝과 치킨 모모를 시켰다. 실수한 것은, 프라이드 모모를 원했는데, 주문을 스팀으로 잘못했다는 건데 뭐 그럭저럭 잘 먹었다. 맛도 역시 그럭저럭, 특별히 감동은 없었다. 이제 시간은 낮 12시. 근데, 저녁 7시까지 뭐하지..? 지금은 16시 25분. 타멜거리를 둘러보기를 잘했다.가장 혼란스러운 거리에서 어쩌면 평화가 조금은 찾아오기도 했다.","categories":[{"name":"Writing","slug":"Writing","permalink":"https://longshiine.github.io/categories/Writing/"}],"tags":[{"name":"Travel","slug":"Travel","permalink":"https://longshiine.github.io/tags/Travel/"},{"name":"Nepal","slug":"Nepal","permalink":"https://longshiine.github.io/tags/Nepal/"}]},{"title":"StarGAN-v2 논문 리뷰","slug":"2021/07/19/StarGAN-v2-review","date":"2021-07-19T05:26:32.000Z","updated":"2023-01-06T06:40:53.275Z","comments":true,"path":"2021/07/19/StarGAN-v2-review/","link":"","permalink":"https://longshiine.github.io/2021/07/19/StarGAN-v2-review/","excerpt":"","text":"StarGAN-v2Github: https://github.com/clovaai/stargan-v2 Paper: https://arxiv.org/abs/1912.01865 Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at this https URL. Abstract 좋은 image-to-image translation model은 다음의 특성을 만족시킨다. 생성된 이미지의 다양성 다양한 도메인으로의 확장성 StarGAN-v2는 Single Framework로 이를 만족시키는데, CelebA-HQ와 AFHQ dataset을 통한 실험은이 모델이 visual quality, diversity, scalability 에 있어 superiority를 가진다는 사실을 validate 하게 한다. 1. Introduction용어정리 Domain: set of images that can be grouped as a visually distinctive category. Style: each image has a unique appearance ex) 이미지의 domain을 사람의 성별로 설정할 수 있음. 이때 style은 ****makeup, beard(턱수염), hairstyle 등을 포함한다. → 위 그림의 celebA-HG 의 예시! 이상적인 i2i 변환 방법은 각 도메인의 서로 다른 스타일을 고려하여 합성할 수 있어야 한다. —&gt; 하지만 너무 많은 임의의 스타일과 도메인이 데이터 셋에 있기 때문에 그런 모델을 만드는 것이 어렵다.. 잠시 살펴보고 가는 쉽게 쓰여진 GANhttps://dreamgonfly.github.io/blog/gan-explained/ 기존의 접근 방식들(methods) 다른 스타일에 접근하기 위해, 많은 방법들은 low-dimentional latent code를 generator에 inject한다. —&gt; uniform or Gaussian distribution에서 랜덤하게 추출된 (100, ) 짜리 벡터 등 그러한 방법들의 domain-specific 디코더는 이러한 latent-vector를 이미지를 생성하는 Generator의 레시피로 사용한다. 그러나, 이러한 method들은 2 domain간의 mapping만을 고려한다. —&gt; # of domain에 있어 not scalable —&gt; K 도메인을 학습하기 위해선 K(K-1)개의 Generator를 학습시켜야함 ⇒ 결국 practical usage가 떨어짐. StarGAN은? 머가 달라? Scalability에 접근하기 위해, unified framed work들이 제안되었는데, starGAN은 초기의 모델중 하나! —&gt; Single generator를 사용하여 모든 가능한 domain의 mapping을 학습가능 Generator는 domain label을 추가적인 input으로 받아 상응하는 도메인으로 변환하는 것을 배움. 그러나, 여전히 StarGAN은 각 도메인의 deterministic mapping만을 배울 뿐, data distribution의 multi-modal적인 특성은 포착하지 못함. —&gt; Multi-modal: 우리의 경험은 실제로 **복합적(multimodal)**인데, 보고, 듣고, 촉감을 느끼고, 향기를 맡고, 맛을 음미하는 것이 그것이다. Modality는 어떤 일이 일어나거나 우리가 무언가를 경험하는 다양한 방식을 말한다. 그리고 이것을 활용하기 위해서는 멀티모달로 특징화 해야한다. —&gt; Multi-modal Data: 서로 다른 형태의 정보로 이루어져 뚜렷한 특성이 구분되는 데이터 이러한 한계는 각 도메인이 이미 결정된 라벨로부터 결정되는 데에서 온다. Generator가 one-hot vector같은 fixed label을 input으로 받는 것을 주목하자. 그렇기에 필연적으로 각 도메인에 대해 같은 output을 낼 수 밖에 없다. StarGAN-v2: 문제에 대한 해법 StarGAN에서 부터 출발하여(scalability에 대한 해법), 그것의 domain label을 새롭게 고안한 domain-specific style code로 변경함(multi-modal nature 파악에 대한 해법) —&gt; style code는 specific한 domain의 구분되는 styles를 표현가능! 이것을 위해 두가지 module을 도입했는데 Mapping Network: 랜덤 Gaussian Noise를 style code로 transform 하는 녀석 Style Encoder: 주어진 reference이미지에서 style code를 추출하는 녀석 **** 이렇게 생성된 style codes를 가지고, Generator는 성공적으로 여러 도메인의 서로 다른 이미지를 합성해내는 것을 배우게 된다! 논문은 우선 StarGAN-v2의 individual component들의 효과를 연구했고, style code를 활용하는 것이 실제로 효과가 있음을 보인다. (Section 3.1 에서) 그리고 제안한 방법(StarGAN-v2)이 multiple domain에 대해 scalable하며, visual quality와 diversity에 있어서도 이전의 방법들보다 나음을 확인했다. 2. StarGAN v22.1 Proposed framework X, Y를 sets of images and possible domain이라고 하자. → X = sets of images, Y = sets of possible domain 이미지 x와, 임의의 도메인 y에 대해 StarGAN-v2의 목적은 single generator G가 x에 상응하는 도메인 y에 대한 구분 가능한 이미지를 생성하게 하는 것이다. —&gt; 어떤 이미지 x가 있고, “금발” 이라는 도메인이 있을때 G가 x의 다양한 금발 버전을 만들어 낼 수 있게 하는 것 —&gt; 근데 이때 아마 위에서 얘기한 multi-modal 적인 data의 특성을 반영하기 위함의 관점에서도 style code를 domain label대신 쓰지 않을까(여성이고, 금발이며, 서양인이고 ~~ 등의 복합적인 도메인으로의 변환) 우리는 사전에 학습된 각 도메인의 style space 내에서 domain-specific style vectors를 생성해 내고, G로 하여금 style vector를 반영하도록 학습시킨다. 아래의 Figure 2가 전체 프레임워크의 overview를 보여준다. 총 4개의 모듈로 구성된다. (a) Generator generator G는 input 이미지 x를 output image G(x,s)로 변환하는데 이때, mapping network F 혹은 style encoder E 중 하나를 통해 생성된 domain-specific style code s를 반영한다. AdaIN(Adaptive instance normalization)을 사용한다. —&gt; to inject s into G 이 “s“가 style of specific domain y를 표현한다는 것을 발견했고, 이에 y(domain 정보)를 G에게 주지 않기 때문에 —&gt; G가 all domain의 image들을 synthesis할 수 있게 되었다!!! —&gt; domain label 대신 style code를 Generator에 input으로 줌으로서 scalability를 챙길 수 있었다! (b) Mapping network 주어진 latent(잠재) code z와 domain y에 대하여, mapping network F는 style code 를 생성한다. F는 모든 가능한 도메인에 대한 style codes를 제공하기 위해 여러 output branch를 가진 MLP로 구성된다. F는 diverse style codes 를 와 에서 randomly하게 sampling 하여 생성할 수 있다. 논문에서 보이는 multi-task architecture는 F로 하여금 효율적이고 효과적으로 모든 domain에 대한style representations 을 학습하게 한다. (c) Style encoder 주어진 image x와 상응하는 domain y에 대하여 encoder E는 style code 를 추출한다. F와 비슷하게, E도 multi-task learning setup에서 이점이 있다. E는 different reference 이미지를 사용하여 diverse style codes를 만들어낸다. 이것은 G로 하여금 reference 이미지 x의 style s를 반영하는 output image를 합성해 낼 수 있게 한다! (d) Discriminator D는 multiple output branches로 구성되는 multi-task discriminator이다. 각각의 branch 는 이미지 x가 domain y 에 해당하는 진짜 이미지인지, fake이미지 인지를 구분하는 binary classification을 학습한다. 2.2 Training Objectives주어진 이미지 , 그에 대한 original domain 에 대해 다음의 목적함수(objectives)를 통해 학습한다. Adversarial objective training 시에, 우리는 latent code 와 target domain 을 randomly sample하고, target style code 를 생성한다. Generator G는 이미지 x와 를 input으로 받아 output image인 를 생성하는 방법을 adversarial loss를 통해 학습한다 는 도메인 y에 상응하는 Discriminator의 output mapping network F는 target domain 에 포함될 것 같은 style code 를 생성하도록 학습이 되고, 는 를 활용하여 domain 의 real image와 indistinguishable(구분불가)한 이미지 를 생성함 Style reconstruction G는 style code 를 활용하게 하기 위해 style reconstruction loss를 사용함 이 objective는 여러 인코더를 사용하여 image → latent code mapping을 학습하는 이전의 접근 방식들과 유사하다. 주목할만한 차이점은 StarGAN-v2에서는 multiple 도메인에 대한 output을 위해 single Encoder E에 대해서만 학습한다는 것! —&gt; 이는 앞서 style code 도입을 통해 얻을 수 있는 이점과 이어진다. At test time, 학습된 encoder E는 G가 input이미지를 reference image의 style을 반영하여 transform할 수 있게 만든다! Style diversification Generator G가 더욱 diverse images를 만들어 낼 수 있게 하기위해, diverse sensitive loss를 통해 명시적으로 regularize(정규화)한다. F가 생성하는 target style codes 와 는 for 에서 비롯된다. regularization term을 최대화 하는 것은 Generator가 diverse images를 생성하기 위해 image 공간을 explore하고 의미있는 style feature를 찾게한다. 주의해야할 것은 기존의 form에서 denominator(분모)의 가 조금의 차이만 발생해도, loss를 크게 increase하는데, 이는 training을 불안정하게 만듬 때문에 논문에서는 denominator part를 제거하고, 안정적인 training을 위한 새로운 equation을 고안함 —&gt; but same intuition Preserving source characteristics 생성된 이미지 가 domain invariant characteristics(eg. pose)를 적절히 보존하도록 보장하기 위해 cycle consistency loss를 고안 는 input 이미지 x의 추정된 style code이고, y는 x의 original domain 이다. G가 input 이미지 x를 reconstruct 할때 estimated style code of input x 를 포함시켜 줌으로서 G는 x의 style을 faithfully(충실히) 변화시키면서도 x의 원래 특성을 보존시킬 수 있다. —&gt; source의 얼굴이 그대로인 이유는 때문이구먼! Full objective term들은 해당 loss의 hyper-parameters 중요! 이 논문에서는 latent-vector(mapping network) 대신 reference image(style encoder)를 사용하여 style code를 generate할 때에도 같은 objective를 사용하였다! —&gt; more training detail은 Appendix B 참고! —&gt; 사실상 위의 에 대한 얘기인데, 이 objective는 latent vector를 style code로 매핑하는 mapping network F 라는 녀석에 대한 loss였는데, —&gt; 이게 ref image를 style code로 변환하는 style Encoder E 에도 똑같이 적용된다는 소리일까..? 3. Experiments StarGAN-v2의 components들(각각의 모듈들)을 실험했고, 3가지의 leading baseline들과 비교했다. 모든 실험은 unseen data에 대해서 이루어졌음 Baselines MUNIT, DRIT, MSGAN 을 baseline으로 삼았고, 이들은 two domain간의 multi-modal mapping을 수행함. Multi-domain에 대한 비교를 위해 이들을 각각의 이미지 domain pairs에 대해 여러번 학습시킴 StarGAN이랑도 비교했는데(v1), 이 모델은 multiple domain에 대한 mapping을 single generator로 배움 Datasets CelebA-HQ와 AFHQ data set에 대해 평가 CelebA-HQ 데이터 셋을 male, female의 두가지 도메인으로 나누었음. domain-label을 제외하고는 추가적인 정보(e.g. facial attributes of CelebA)를 사용하지 않았으며, 모델로 하여금 비지도적으로 style들에 대한 정보를 배우도록 했음. 공정한 비교를 위해 훈련시 이미지를 256X256 으로 resize했고, 이 사이지는 baseline들의 최대 resolution 사이즈임. Evaluation metrics 우선 평가해야 할 metric은 두 가지인데, 1. visual quality —&gt; **FID**(Frechet Inception distance): distance between two distributions of real and generated images (lower is better) Q. FID가 visual quality에 대한 지표도 포함하는가..? diversity of generated images —&gt; LPIPS(Learned Perceptual Image Patch Similarity): (higher lis better) 위 표는 StarGAN-v1에 v2의 모듈 혹은 기법들을 한 개씩 추가하며 measure한 것! (A): StarGAN-v1의 결과 —&gt; 메이크업만을 옮겨 local change만 이끌어낼 뿐임 (B): ACGAN Discrimintor를 multi-task discriminator로 변경 —&gt; generator가 input이미지의 global structure를 변경할 수 있게함. (C): R1 regularization &amp; AdaIN 적용 —&gt; training stablility를 증가시킴 —&gt; (A),(B),(C)의 경우 주어진 input image와 target domain에 대해서 multiple output을 낼수 없기 때문에, LPIPS 측정이 불가하다. ⇒ 1장에서 다양성을 measure할 수 없으므로 (D): diversity(다양성)을 이끌어내기위해 latent code z를 generator G에 바로 넣는다는 생각을 할 수 있음 —&gt; 실험해본 결과, multi-domain 시나리오에서 이러한 baseline는 네트워크로 하여금 의미있는 styles를 배우게 하지 못하고, 기대하는 diversity를 내지 못했다. 때문에, 논문에서는 latent code가 domain을 구분 하는데에 능력이 없고, latent reconstruction loss는 domain-specific 한 style 보다 domain-shared style을 모델링할 것이라 추측했다. G에 latent code를 direct로 주기보단, mapping network F를 통해 z에서 domain-specific style code s를 생성하여 G에 inject함 style reconstruction loss 도 소개함 주목해야할 것은, mapping network의 각각의 output branch가 특정한 domian 에 대한 것이라는 것이다. 때문에 stylecode는 domain을 구분하는 것에 대한 ambiguity(모호성)이 없다. latent reconstruction loss(F에 대한 Loss)와 다르게, style reconstruction loss는(E에 대한 Loss) generator로 하여금, domain-specific style을 반영하여 이미지를 생성하게 한다. —&gt; (reference image의 style code를 사용하여 이미지를 생성한다는 뜻인듯) 3.2 Comparison on diverse image synthesis 두가지 관점에서 비교할것 latent-guided synthesis —&gt; baseline들과 비교할 때, 유일하게 안깨짐 암튼 base들보다 좋다. reference-guided synthesis —&gt; ref image로 부터 추출된 style code를 사용함으로 distinctive style을 가져올 수 있었으며, color distribution만 추출해내는 기존 기법들과 차이를 보였다. Human-evaluation 사람들에게 투표시킬 때도 baseline 보다 나았다. 4. DiscussionFirstmulti-head mapping network, style encoder 에 의해서 style code가 도메인에 대해 각각 생성된다. —&gt; Generator가 stylecode를 사용하는 것에만 초점을 맞출 수 있다. Second기존 baseline들은 style space를 fixed Gaussian distribution을 따른다는 가정을 했으나, StarGAN-v2에서는 StyleGAN의 insight에 따라, transformation에 대해 배운 style space에서 생성해냄 FFHQ에 대해서도 Reference-guided synthesis가 좋은 결과를 냈다. 5. Related work StyleGAN: non-linear mapping function을 도입하여, imput latent code를 style space로 매핑함. —&gt; 그러나 styleGAN은 image를 input으로 받지않아서, real image를 transform 한다라는 task가 자명하지 않다. 본 논문의 latent-guided synthesis와 reference-guided synthesis 모두 coarsely(거칠게) labeled dataset으로 train 되었다. 1234567891011결국 reference guided synthesis로 갈텐데,이때 우리는 스타일 합성을 하는 것이지, 헤어 합성이 아니다.—&gt; 초기에는 스타일 합성 기능을 제공하는 것으로,,?—&gt; 대신 이를 위해 동양인 dataset에 대한 fine tuning이 필요함.-&gt; celebA-HQ 30000장, 1024x10241. 초기 launching시에 우리가 제공하는 스타일들을 보여주고,10개정도 scrap할 수 있게(왓챠나 넷플릭스 처럼)함.2. 우리는 AI 스타일 합성을 제공하며, 이는 해당 사진의 헤어, 메이크업 등을 포함함3. 따라서 우리는 스타일 전반에 대한 설명 또한 제공(메이크업 정보, 헤어 정보 등) 6. Conclusion StarGAN-v2는 one domain to diverse images of a target domain and supporting multiple target domain AppendixB. Training details batch size: 8 iteration: 100,000(100K) Tesla V100 GPU 1개, 3 days for CelebA-HQ non-saturating adversarial loss + R1 regularization Adam optimizer with Learning rates for Learning rates for initialize the weights = He initialization set biases to zero —&gt; except for biases associated with scaling vector of AdaIN(set to one) E. Network architectureGenerator Mapping Network —&gt; K output branches가 있는데, 이는 domain의 개수를 뜻함. —&gt; 첫 네 개의 FC는 domain간 공유되고, 뒤의 네 개는 domain별로 존재. —&gt; 각각의 dimention 정보 latent code: 16 hidden layer: 512 style code: 64 —&gt; latent code는 standard Gaussian distribution에서 sample —&gt; pixel normalization을 latent code에 적용하지 않았다.(model performance를 올리지 못하는 것으로 밝혀짐) —&gt; feture normalization도 시도해봤으나, 이도 잘 안되는 것으로 파악 Style encoder —&gt; CNN with K output branch —&gt; 6개의 pre-activation residual block이 domain간에 공유 —&gt; Dimension 정보 Output: 64 x K Discriminator multi task discriminator로서 위의 테이블 공유 K개의 FC, for real/fake classification of each domain —&gt; Dimension 정보 D = 1 (real/fake) multi-task discriminator가 다른 conditional discriminators 보다 나음을 확인 Fine Tuning 관련 Azure V100 x 8개 —&gt; 30,000장 학습, 1개, 3일(72시간) ⇒ 8개, 9시간 2725 x 9 = 24,000원 V100 else","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"딥러닝","slug":"Tech/딥러닝","permalink":"https://longshiine.github.io/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"},{"name":"ComputerVision","slug":"ComputerVision","permalink":"https://longshiine.github.io/tags/ComputerVision/"},{"name":"GAN","slug":"GAN","permalink":"https://longshiine.github.io/tags/GAN/"}]},{"title":"Priority Queue(우선순위큐)","slug":"2021/02/05/Priority-Queue","date":"2021-02-05T12:46:32.000Z","updated":"2023-01-06T06:39:15.304Z","comments":true,"path":"2021/02/05/Priority-Queue/","link":"","permalink":"https://longshiine.github.io/2021/02/05/Priority-Queue/","excerpt":"","text":"프로그래머스: 이중우선순위 큐 풀이코테 대비 후루룩 훑기(2)https://programmers.co.kr/learn/courses/30/lessons/42628 123456789101112131415161718192021222324252627def solution(operations): import heapq length = 0 heap = [] for operation in operations: oper = operation.split() if oper[0] == &quot;I&quot;: heapq.heappush(heap, int(oper[1])) length += 1 if oper[0] == &quot;D&quot; and len(heap) != 0: if oper[1] == &quot;-1&quot;: heapq.heappop(heap) length -= 1 else: # 여기가 중요한데, heapq는 최소힙이므로, # 최댓값을 찾기 위해서, 힙에 있는 요소들을 pop한뒤 새로운 heap에 push한다. # 단, 마지막에 pop 되는 값이 최댓값이므로 이 값을 버린다. new_heap = [] for _ in range(length-1): heapq.heappush(new_heap, heapq.heappop(heap)) heap = new_heap length -= 1 if length != 0: answer = [max(list(heap)), min(list(heap))] else: answer = [0,0] return answer heapq 모듈 정리 heapq는 기본적으로 최소힙 을 지원 heapq.heappush(heap, node): 힙에 요소를 추가 heapq.heappop(heap): 힙에서 최솟값을 제거 (이때 자동으로 힙 정렬이루어짐)\u001d heapq.heapify(list): 리스트를 heap으로 만들어줌","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"알고리즘","slug":"Tech/알고리즘","permalink":"https://longshiine.github.io/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://longshiine.github.io/tags/Algorithm/"},{"name":"PriorityQueue","slug":"PriorityQueue","permalink":"https://longshiine.github.io/tags/PriorityQueue/"},{"name":"heapq","slug":"heapq","permalink":"https://longshiine.github.io/tags/heapq/"}]},{"title":"DFS와 BFS","slug":"2021/02/05/DFS와-BFS","date":"2021-02-05T04:50:29.000Z","updated":"2023-01-06T06:44:11.372Z","comments":true,"path":"2021/02/05/DFS와-BFS/","link":"","permalink":"https://longshiine.github.io/2021/02/05/DFS%EC%99%80-BFS/","excerpt":"","text":"백준 1260번 문제: DFS와 BFS 풀이코테 대비 후루룩 훑기(1)https://www.acmicpc.net/problem/1260 그래프 만들기1234567891011121314if __name__ == &quot;__main__&quot;: from sys import stdin N, E, begin = map(int, stdin.readline().rstrip().split()) # 입력 첫째줄 (노드개수, 간선개수, 시작점) graph = &#123;node: [] for node in range(1, N+1)&#125; i = 0 while i &lt; E: i += 1 node, edge = map(int, stdin.readline().rstrip().split()) graph[node].append(edge) graph[edge].append(node) print(dfs(graph, begin)) print(bfs(graph, begin)) DFS1234567891011def dfs(graph, start_node): for n in graph: graph[n] = sorted(graph[n], reverse=True) visit = list() stack = list() stack.append(start_node) while stack: node = stack.pop() if node not in visit: visit.append(node) stack.extend(graph[node]) return visit BFS1234567891011def bfs(graph, start_node): for n in graph: graph[n] = sorted(graph[n]) visit = list() stack = list() stack.append(start_node) while stack: node = stack.pop(0) if node not in visit: visit.append(node) stack.extend(graph[node]) return visit 백준에서 파이썬 입출력 받기1234567891011if __name__ == &quot;__main__&quot;: from sys import stdin # .split()으로 input으로 들어오는 line을 # 공백 단위의 리스트로 만들어주고, # .map(func, list)으로 각 데이터를 처리 # 이때 반환되는 것은 &quot;맵 객체&quot; 이고, 여러 변수로 받을 수 있음 N, M = map(int, stdin.readline().rstrip().split()) while i &lt; N: # 뭐 이런식으로 입력 더 받으면 됨 A, B = map(int, stdin.readline().rstrip().split()) i += 1","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"알고리즘","slug":"Tech/알고리즘","permalink":"https://longshiine.github.io/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://longshiine.github.io/tags/Algorithm/"},{"name":"DFS","slug":"DFS","permalink":"https://longshiine.github.io/tags/DFS/"},{"name":"BFS","slug":"BFS","permalink":"https://longshiine.github.io/tags/BFS/"}]},{"title":"2021년 1월","slug":"2021/01/31/2021-01","date":"2021-01-31T04:42:50.000Z","updated":"2023-01-06T06:44:56.866Z","comments":true,"path":"2021/01/31/2021-01/","link":"","permalink":"https://longshiine.github.io/2021/01/31/2021-01/","excerpt":"","text":"1월 2일 (토)생각을 정리해야 할 중요한 일이 될만한 것은 일상에 관한 것이다. 솔직히 요새 해야만 하는 중요한 일이란 것은 연구실의 논문을 읽고 정리하는 것 뿐이다. 고작 하나짜리 논문과 낑낑대고 있다는 생각을 자꾸만 한다. 하지만 진지하게 두고 보면 나의 실력이 그정도에 머물고 있고, 이 일이 주어진 일이라면 해내기만 하면 된다. “수처작주 입처개진(隨處作主 立處皆眞)” 머무르는 곳마다 주인이 되어라. 지금 있는 그곳이 진리의 자리이다. 내가 있는 이곳의 주인이 되어야 한다. 고민이 될때면, 내 자리에서 묵묵하게 주인이 먼저되자. 1월 5일 (화)노력한다라고 스스로 생각할 때면, 자기 자신에게 도취되는 경향이 있다. 그러한 경향은 이따금씩 이상한 곳으로 나를 이끌게 한다. 공허만 남는다거나 그런. 내가 지금 무언가 “노력”할 대상을 찾는 것은 그것이 일련의 구원이 되어줄 것이라는 생각에서 비롯되었을 가능성이 크다. 노력하여 이룬 성취가 결국 나를 증명해줄 것이고, 그것에 안주할 수 있을 것이라는 생각. 사실은 그렇지 않다. 성취 또한 일련의 중독일 수 있다. 매번 그 도취감에 기대어 다른 방식의 삶을 무시한다면, 매번 이렇게 공허에 시달릴 가능성이 크다. 1월 7일 (목)일상을 되찾고 싶다. 누군가에겐 의무랄 것이 없는 내 삶이 부러울 테지만, 사실은 그렇지 않다. 일이 없는 삶은 너무도 무료하고, 부질없이 흘러만 간다. 일로써 글을 쓰는 것은 어떠한가. 모든 것들로 부터 멀어져간다. 지나온 것들을 긍정해야 한다라고 스스로 되새겨보지만 쉽지가 않다. 나느 아무래도 나의 일을 하며 살고 싶다. 코딩이라던지. 그럼 실천해야한다. “목표”까지 세우기가 벅차다면 매일매일 해야할 것이라도 수립해보자. 1월 14일 (목)“아무도 읽지 않는다는 이유로 장문의 글을 쓰지 않다보면 어느 새벽, 당신은 읽는 이가 기다린대도 긴 글을 쓸 수 없게 되었음을 깨닫게 된다. 아무도 먹어주지 않는다는 이유로 요리하지 않다보면 혼자만의 식사도 거칠어진다. 당신의 우주는 그런식으로 비좁아져간다.” 나의 우주가 정말이지 비좁아져 가는 느낌이다. 말그대로 읽히지 않는 글을 쓰지 않고, 혼자 먹는 음식을 요리하지 않으며, 혼자 들을 기타를 연습하지 않다보니 나의 우주가 비좁아져 가는 느낌이다. 1월 22일 (금)정답은 없다. 결국엔 마음 가는대로, 일어나는 대로. 도망쳐도 괜찮고, 버텨도 좋다. 도망치는건 부끄럽지만 도움이 된다. 시대가 그렇다. 정답이 없는 시대다. 물려받은 자유의 시대이다. 자유를 누리려면 그에 따르는 합당한 책임을 져야 한다. 그것은 첫째로 소음을 듣는 일이고, 둘째로 시간에 몸을 맡기는 일이다. 누구나 자유롭기에 소음은 생길 수 밖에 없고, 스스로가 자유롭기에 정해진 의미랄건 없다. 시간에 몸을 맡긴다. 구체적으로는 어떻게? 1월 27일 (수)분명 느려도 좋고, 시작이라는 것은 늘 설레는 일이라는 것을 스스로 생각하지만, 나의 맥락에 놓여질때에는 여간 쉬운일이 아닌 것 또한 잘 알겠다. “하면 할 수 있다.” 이상하지 만치 이 문장에는 알게 모르게 자신이 있다. 다만 하지 않을 뿐. 하고 싶은 것을 찾지 못했을 뿐. 다 애매하다랄까. 애매하게 잘하는 것은 다소간의 저주일지 모르겠다. 보통의 이야기들에서 저주를 풀어내는 방법은 비슷하다. 모험을 떠나는 것.","categories":[{"name":"Writing","slug":"Writing","permalink":"https://longshiine.github.io/categories/Writing/"}],"tags":[{"name":"일기","slug":"일기","permalink":"https://longshiine.github.io/tags/%EC%9D%BC%EA%B8%B0/"}]},{"title":"강화학습 기초3: 살사(SARSA)와 큐러닝(Q-Learning)","slug":"2021/01/27/강화학습-기초6","date":"2021-01-27T02:43:24.000Z","updated":"2023-01-06T06:24:05.454Z","comments":true,"path":"2021/01/27/강화학습-기초6/","link":"","permalink":"https://longshiine.github.io/2021/01/27/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EC%B4%886/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 일곱번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 4장 강화학습 기초 3: 강화학습 알고리즘정책 이터레이션과 가치 이터레이션은 살사로 발전한다. 살사부터 강화학습이라 부른다. 각 이터레이션들이 어떻게 살사로 발전하는지, 살사와 같은 강화학습 알고리즘을 통해 에이전트가 어떻게 학습하는지 알아보자. 살사(SARSA)정책 이터레이션은 정책 평가 와 정책 발전 을 번갈아 가며 실행하는 과정이다. 벨만 기대 방정식을 이용해 현재의 정책에 대한 참 가치함수를 구하는 것이 정책 평가이며, 구한 가치함수에 따라 정책을 업데이트 하는 것이 정책 발전이다. 이러한 정책 이터레이션을 GPI(Generalized Policy Iteration)이라고 한다. GPI에서는 단 한번만 정책을 평가해서 가치함수를 업데이트하고 바로 정책을 발전하는 과정을 반복한다. GPI에서는 벨만 방정식에 따라 정책을 평가한다. 그 대신 강화학습에서는 몬테카를로 예측 이나 시간차 예측 을 사용하여 정책을 평가한다. GPI의 탐욕 정책 발전 은 주어진 가치함수에 대해 모든 상태에 대한 정책을 얻는 과정이다. 시간차 방법 에서는 타임스텝마다 가치함수를 현재 상태에 대해서만 업데이트 하므로, 모든 상태의 정책을 발전시킬 수 없다. 때문에 시간차(Temporal-Difference) 방법에서는 가치 이터레이션의 방법을 도입한다. 가치 이터레이션에서는 정책 이터레이션과는 달리 별도의 정책 없이 가치함수에 대해 탐욕적으로 움직일 뿐 이었고, 시간차 방법에서도 에이전트는 현재 상태에서 가장 큰 가치를 지니는 행동을 선택하는 탐욕 정책 을 사용한다. 시간차 예측과 탐욕 정책이 합쳐진 것을 시간차 제어(Temporal-difference control)이라고 한다. 이들의 관계를 나타낸 것이 아래의 그림이다. 탐욕 정책에서 다음 상태의 가치함수를 보고 판단하는 것이 아닌 현재 상태의 큐함수를 보고 판단한다면 환경의 모델을 몰라도 된다. (GPI의 탐욕 정책 발전에 따르면 을 계산하기 위해서 상태 변환 확률인 를 알아야 했다. 행동을 했을 때, 이상한데로 튈 확률이랄까나. 요녀석은 환경의 일부로서 현실에서는 알기가 매우매우 힘든 정보이기 때문에, 이 녀석 없이 행동을 선택하게 하는 것이 필요하다!)시간차 제어에서는 아래의 큐함수를 사용한 탐욕 정책 을 통해 행동을 선택한다. 큐함수에 따라서 행동을 선택하려면 에이전트는 가치함수가 아닌 큐함수의 정보를 알아야 하므로, 업데이트의 대상은 가치함수가 아닌 큐함수가 되어야 한다. 때문에 시간차 제어의 식은 다음과 같다. 시간차 제어에서 큐함수를 업데이트 하려면 샘플이 필요하다. 시간차 제어에서는 을 샘플로 사용한다. 흐름을 한번 살펴보면 에이전트는 에서 탐욕 정책에 따라 를 선택 환경은 을 제공하고 다음상태 을 제공 에이전트는 에서 탐욕 정책에 따라 을 선택 샘플의 형태 때문에 시간차 제어를 다른말로 살사(SARSA)라고 부른다. 살사는 현재 가지고 있는 큐함수 를 토대로 샘플을 탐욕 정책으로 모으고, 그 샘플로 방문한 큐함수를 업데이트하는 과정을 반복하는 것이다. 초기의 에이전트에게 탐욕정책은 잘못된 학습으로 가게할 가능성이 크다. 때문에, 큐함수가 잘못된 값에 수렴하는 것을 막기 위해 에이전트가 충분히 다양한 경험을 하도록 해야하고, 이를 위해 -탐욕 정책 을 사용한다. 간단한 아이디어인데 의 확률로 탐욕적이지 않은 행동을 선택하게 하는 것이다.(물론 -탐욕 정책은 최적 큐함수를 찾았다 하더라도 의 확률로 계속 탐험한다는 한계 가 있다. 따라서 학습을 진행함에 따라 값을 감소시키는 방법을 사용할 수도 있다.) 의확률로의확률로 정리하자면, 살사는 간단히 두 단계로 생각하면 된다. -탐욕 정책을 통해 샘플 을 획득 획득한 샘플로 다음 식을 통해 큐함수 를 업데이트 이제 코드를 한번 살펴보자 살사 코드 설명1234567class SARSAgent: def __init__(self,actions): self.actions = actions # 에이전트가 할 수 있는 행동 [상,하,좌,우] self.step_size = 0.01 # α self.discount_factor = 0.9 # γ self.epsilon = 0.1 # ϵ self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0]) init을 통해 학습에 필요한 변수들을 할당해 주었다. 추가로 SARSAgent 에 어떤 함수가 필요한지를 알기 위해서는 에이전트가 환경과 어떻게 상호작용하고 학습하는지를 알아야 한다. 에이전트는 다음과 같은 순서로 상호작용한다. 현재 상태에서 -탐욕 정책에 따라 행동을 선택 선택한 행동으로 환경에서 한 타임스텝을 진행 환경으로부터 보상과 다음 상태를 받음 다음 상태에서 -탐욕 정책에 따라 다음 행동을 선택 (s,a,r,s’,a’)을 통해 큐함수 업데이트 get_action 함수는 -탐욕 정책에 따라 state를 입력으로 받아 action을 반환한다. q_table에 따라서 탐욕적으로 행동을 선택하며 이때 의 확률을 반영하여 무작위 행동을 반환하기도 한다. 1234567891011# 입실론 탐욕 정책에 따라서 행동을 반환def get_action(self, state): if np.random.rand() &lt; self.epsilon: # 무작위 행동 반환 action = np.random.choice(self.actions) else: # 큐함수에 따른 행동 반환 state = str(state) q_list = self.q_table[state] action = arg_max(q_list) return action 현재 상태와 다음 상태에서의 행동을 선택해서 샘플(s,a,r,s’,a’)을 얻으면 에이전트는 학습을 진행한다. 즉 아래의 식의 역할을 하는 함수는 learn이며 코드는 다음과 같다.(굉장히 직관적이다!) 12345678# &lt;s, a, r, s', a'&gt;의 샘플로부터 큐함수를 업데이트def learn(self, state, action, reward, next_state, next_action): state, next_state = str(state), str(next_state) current_q = self.q_table[state][action] next_state_q = self.q_table[next_state][next_action] td = reward + self.discount_factor * next_state_q - current_q new_q = current_q + self.step_size * td self.q_table[state][action] = new_q get_action과 learn 함수를 통해 에이전트는 메인 루프에서 다음과 같이 환경과 상호작용한다. 123456789# 행동을 위한 후 다음상태 보상 에피소드의 종료 여부를 받아옴next_state, reward, done = env.step(action)# 다음 상태에서의 다음 행동 선택next_action = agent.get_action(next_state)# &lt;s,a,r,s',a'&gt;로 큐함수를 업데이트agent.learn(state, action, reward, next_state, next_action)state = next_stateaction = next_action 살사의 한계살사에서는 충분한 탐험(Exploration) 을 하기 위해 -탐욕 정책을 사용했다. 그런데 다음의 경우를 한번 생각해보자. 초기 에이전트가 만약 s에서 a라는 행동을 하고 다음 행동인 a’은 탐험을 통해서 가게 되었다고 생각해보자. 그럼 자연스럽게 초기 에이전트는 Q(s,a) 값을 낮출 것이고 이에 따라 s에서 아래로 이동하는 행동이 안좋다고 판단할 것이다. 결국 에이전트가 특정 state에 갇혀버리는 현상 이 발생한다. 이렇게 자신이 행동한 대로 학습하는 것을 On-Policy 시간차 제어 라고 한다. 이러한 딜레마를 해결하기 위해 사용하는 것이 바로 오프폴리시 시간차 제어 , 큐러닝 이다. 큐러닝큐러닝의 아이디어는 간단하다. 오프폴리시 의 말 그대로 현재 행동하는 정책과는 독립적으로 학습한다는 것이다. 즉, 행동하는 정책과 학습하는 정책을 따로 분리 한다. 이게 무슨 말일까? 예시로서 이해해보자. 에이전트가 현재 상황 에서 행동 를 -탐욕 정책에 따라 선택했다고 하자. 그러면 에이전트는 환경으로부터 보상 을 받고 다음 상태 을 받는다. 여기까지는 살사와 동일하다. 하지만 살사에서는 다음 상태 에서 또다시 -탐욕 정책에 따라 다음 행동을 선택한 후에 그것을 학습에 샘플로 사용한다. 큐러닝 에서는 에이전트가 다음 상태 을 알게 되면 그 상태()에서 가장 큰 큐함수를 현재 큐함수의 업데이트에 사용한다. 살사의 학습과정과 다르게 큐러닝은 아래와 같이 학습한다. 큐러닝은 실제 다음 상태 에서 다음 행동을 해보는 것이 아니라 다음 상태 에서 가장 큰 큐함수를 가지고 업데이트 하는 것이다. 자세히 살펴보면 벨만 최적 방정식과 비슷하다는 생각이 들 것이다. 벨만 최적 방정식은 아래의 수식과 같은데 큐러닝에서 보상 은 실제 에이전트가 환경에게서 받는 값이므로 기댓값을 빼면 동일하다. 벨만 기대 방정식 –&gt; 정책 이터레이션 –&gt; 살사 벨만 최적 방정식 –&gt; 가치 이터레이션 –&gt; 큐러닝 큐러닝은 샘플로서 [s,a,r,s’]을 사용하며 실제 환경에서 행동을 하는 정책과 큐함수를 업데이트할 때 사용하는 정책이 다르기 때문에 큐러닝을 오프폴리시라고 한다. 다른 오프폴리시 강화학습과 달리 큐함수가 간단하기 떄문에 이후에 많은 강화학습 알고리즘의 토대가 되었다. 큐러닝 코드 설명큐러닝 코드에서 살사 코드에서와 다른 점은 에이전트가 샘플을 가지고 학습하는 부분이다. 1234567# &lt;s, a, r, s'&gt; 샘플로부터 큐함수 업데이트 def learn(self, state, action, reward, next_state): state, next_state = str(state), str(next_state) q_1 = self.q_table[state][action] # 벨만 최적 방정식을 사용한 큐함수의 업데이트 q_2 = reward + self.discount_factor * max(self.q_table[next_state]) self.q_table[state][action] += self.step_size * (q_2 - q_1) learn 코드는 위에서 살펴본 큐러닝의 업데이트 식을 구현한 것이다. self.q_table[next_state]에서 max 값을 업데이트에 사용하기 때문에 오프폴리시가 됩니다. 또한 max값을 취하면 되기 때문에 다음 상태에서의 행동을 알 필요가 없다. 살사와 큐러닝의 차이는 온폴리시와 오프폴리시 의 차이라고 볼 수 있다. 온폴리시인 살사는 지속적인 탐험 으로인해 그리드월드 예제에서 왼쪽 위에 갇히곤 하지만 큐러닝은 현재 행동하는 정책과는 독립적으로 학습을 진행하기 때문에 갇히지 않고 벗어나는 정책을 학습할 수 있다. 정리 몬테카를로 예측: 기댓값을 샘플링을 통한 평균으로 대체 시간차 예측: 몬테카를로 예측과는 달리 타임스텝마다 큐함수를 업데이트 살사(SARSA): 강화학습 제어에서 큐함수를 사용하며 하나의 샘플로 (s,a,r,s’,a’)을 사용하는 시간차 제어 큐러닝(Q-learning): 오프폴리시 강화학습으로서 행동선택은 -탐욕 정책 , 큐함수의 업데이트에는 벨만 최적 방정식 사용 4장 한줄평 절반을 넘겼으니 꼭 끝까지 가봅시다","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"SARSA","slug":"SARSA","permalink":"https://longshiine.github.io/tags/SARSA/"},{"name":"QLearning","slug":"QLearning","permalink":"https://longshiine.github.io/tags/QLearning/"}]},{"title":"강화학습 기초: 강화학습과 정책평가","slug":"2021/01/25/강화학습-기초5","date":"2021-01-25T04:10:37.000Z","updated":"2023-01-06T06:22:30.478Z","comments":true,"path":"2021/01/25/강화학습-기초5/","link":"","permalink":"https://longshiine.github.io/2021/01/25/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EC%B4%885/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 여섯번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 4장 강화학습 기초 3: 강화학습과 정책평가강화학습과 다이내믹 프로그래밍의 차이는 강화학습은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적 정책을 학습한다는 것이다. 이때 에이전트는 환경과의 상호작용을 통해 주어진 정책에 대한 가치함수를 학습 할 수 있는데 이를 예측이라고 한다. 또한 가치함수를 토대로 정책을 끊임없이 발전시켜 나가서 최적 정책을 학습 하려는 것이 제어이다. 예측 에는 몬테카를로 예측과 시간차 예측이 있으며, 제어 에는 시간차 제어인 살사가 있고, 살사의 한계를 극복하기 위한 오프폴리시 제어인 큐러닝이 있다. 사람의 학습 방법과 강화학습의 학습 방법강화학습의 에이전트는 어떻게 모델 없이 순차적 행동 결정 문제 를 풀 수 있을까? 강화학습의 학습 은 어떻게 작동하는 것일까?🧐 다이내믹 프로그래밍을 한번 살펴보자. 다이내믹 프로그래밍은 상태나 차원이 증가할수록 계산 복잡도가 기하급수적으로 증가하는데, 이는 환경에 대한 정확한 정보를 가지고 모든 상태에 대해 동시에 계산을 진행하기 때문이다. 마치 바둑을 둘때 모든 경우의 수를 고려하여 두는 형상이다. 사람도 그럴까? 사람도 바둑을 둘 때, 모든 경우의 수를 고려하여 두는 것일까? 아니다! 사람은 학습의 많은 부분을 그냥 바둑을 두면서 진행한다. 후에 복기를 하면서 어디서 잘못을 했고, 어떻게 고쳐야 할까를 고민한다. 여기에 강화학습이 학습을 어떻게 하는가에 대한 답이 있다. 강화학습 은 일단 해보고, 자신을 평가한뒤, 평가한 대로 자신을 업데이트 하며,이 과정을 무수히 반복한다. 강화학습의 예측과 제어MDP로 정의되는 문제를 풀 때 중요한 것은 벨만 기대 방정식의 기댓값인 를 어떻게 계산하는가 이다. 정책 이터레이션 대로 계산을 반복한다면 한 치의 오차도 없는 정확한 기댓값을 얻을 수 있으나, 이러한 다이내믹 프로그래밍을 적용할 수 있는 문제는 많지 않다. 사람은 어떤것을 판단할 때, 항상 정확한 정보를 근거로 판단하지 않는다. 이터레이션보다 정확하진 않지만 적당한 추론 을 통해 학습을 해나가는 것이 실제 세상에서는 더 효율적 이다. 강화학습에서는 적당한 추론을 통해 원래 참 가치함수의 값을 예측(prediction)한다. 강화학습에서는 예측과 함께 정책을 발전시키는 것을 제어(Control)이라 부른다. 몬테카를로 근사의 예시보통 원의 넓이를 계산하기 위해서는 원의 방정식()을 찾아 그 방정식을 이용해 계산한다. 근데, 원의 방정식을 모른다면? 넓이를 어떻게 계산해야 할까. 이때 사용할 수 있는 방법이 몬테카를로 근사(Monte-Carlo Approximation)이다. 몬테카를로라는 말은 무작위로 무엇인가를 해본다는 의미이며, 근사라는 것은 원래의 값은 모르지만 샘플을 통해 원래의 값이 이럴것이다라고 추정하는 것이다. 즉 무작위로 무엇인가를 해서 원래의 넓이를 추정하는 것 이 몬테카를로 근사이다. 몬테카를로 근사로 원의 넓이를 계산하는 예제를 한번 보자. 원(A)이 그려진 네모난 종이(B) 위에 점을 뿌린다고 생각해보자. 전체 뿌린 점들 중에서 A에 들어가 있는 점의 비율을 구하면 이미 알고 있는 를 통해서 의 값을 추정할 수 있다. 더 많은 샘플을 사용할수록 오차는 적어지며 무한히 반복하면 원래의 값과 동일해진다. 원의 넓이의 방정식을 알면 한번에 구할 수 있는데, 왜 이런 고생을 할까?😰 몬테카를로 근사의 장점은 방정식을 몰라도 원래 값을 추정할 수 있다 는 것이다. 어떤 도형이든 상관없이 모든 도형의 넓이를 정확하진 않더라도 구할 수 있다. 아래와 같은 그림도 말이다!(아래의 그림에 대한 도형의 방정식은 존재하지 않는다). 이처럼 방정식을 몰라도 반복하기만 하면 답을 구할 수 있다는 장점은 강화학습에 그대로 이용된다. 몬테카를로 예측(Monte-Carlo Prediction)원의 넓이를 추정하는 대신 가치함수를 추정해보자. 원의 넓이를 추정할 때는 점이 하나의 샘플 이며 점을 찍는 것이 샘플링(sampling) 이었다. 가치함수를 추정할 때는 에이전트가 한 번 환경에서 에피소드를 진행하는 것 이 샘플링이다. 샘플의 평균으로 참 가치함수의 값을 추정할 것인데, 이때 몬테카를로 근사를 사용하는 것을 몬테카를로 예측(Montecarlo Prediction)이라고 한다. 정책 이터레이션은 벨만 기대 방정식의 기댓값이란 녀석을 실제로 계산한 것이다. 샘플링을 통해 기댓값을 계산하지 않고 샘플들의 평균으로 가치함수를 예측 하려면 어떻게 해야할까? 몬테 카를로 예측에서는 환경의 모델을 알아야 하는 를 계산하지 않는다. 환경의 모델을 몰라도 여러 에피소드를 통해 구한 반환값의 평균 을 통해 를 추정한다. 한 번의 에피소드로는 추정이 불가능하다. 그것은 마치 원의 넓이를 구할 때 점을 한번 찍는 것과 같으며 몬테카를로 예측을 하기 위해서는 각 상태에 대한 반환값들이 많이 모여야 한다. 각 상태에 모인 반환값들의 평균을 통해 참 가치함수의 값을 추정한다. 현재 정책에 따라 무수히 많은 에피소드를 진행해 보면 현재 정책을 따랐을 때 지날 수 있는 모든 상태에 대해 충분한 반환값 들을 모을 수 있다. 따라서 상당히 정확한 가치함수의 값을 얻을 수 있다. 반환값들의 평균을 취하는 식을 조금 자세히 살펴보자. 편의상 상태에 대한 표현을 생략한다. n개의 반환값을 통해 평균을 취한 가치함수를 이라고 하는데 여기서 대문자로 표현하는 이유는 오차가 내포되었다는 의미이다. 업데이트식을 정리해가는 과정은 다음과 같다. 어떤 상태의 가치함수는 샘플링을 통해 에이전트가 그 상태를 방문할 때마다 업데이트하게 된다. 원래 가지고 있던 가치함수 값 에 를 더함으로써 업데이트 하는 것이다. 이렇게 시간에 따라 평균을 업데이트 해나가는 것 을 이동평균이라고 하며, 아래는 가치함수의 업데이트 식이다. 위의 수식에서 를 오차라고 하며 을 스텝사이즈(Step size)로서 업데이트할 때 오차의 얼마를 가지고 업데이트 할지를 결정한다. 일반적으로 스텝사이즈 는 라고 표현하며, 위의 식을 일반적인 형태로 나타내면 아래와 같다. 스텝사이즈가 클수록 과거에 얻은 반환값을 지수적으로 감소시킨다. (환경이 지속적으로 변화한다면 1/n로 평균을 구하는 것보다 일정한 숫자로 고정하는 것이 좋다(?)) 몬테카를로 예측에서 에이전트는 이 업데이트 식을 통해 에피소드 동안 경험했던 모든 상태에 대해 가치함수를 업데이트한다. 어떠한 상태의 가치함수가 업데이트 될수록 가치함수는 현재 정책에 대한 참 가치함수에 수렴해간다. 이후의 모든 강화학습 방법에서 가치함수를 업데이트하는 것은 위의 수식의 변형일 뿐이다. 따라서 이 식을 정확하게 이해하는 것이 중요하다. 한 에피소드는 빨간색 선과 같으며, 에이전트는 마침 상태에 갈 때까지 아무것도 하지 않는다. 마침 상태(파란색 동그라미)에 도착하면 에이전트는 지나온 모든 상태의 가치함수를 업데이트 한다. 에피소드 동안 방문했던 모든상태의 가치함수를 업데이트하면 에이전트는 다시 시작부터 새로운 에피소드를 진행하며, 이러한 과정을 계속 반복하는 것이 몬테카를로 예측 이다. 시간차 예측(Temporal-Difference Prediction)몬테카를로 예측의 단점은 실시간이 아니라는 점 이다. 가치함수를 업데이트하기 위해 에피소드가 끝날 때까지 기다려야 한다. 또한 에피소드의 끝이 없거나, 에피소드의 길이가 긴 경우 에는 몬테카를로 예측이 적합하지 않다. 시간차 예측(Temporal-Difference Prediction)은 몬테카를로 예측과는 다르게 타임스텝마다 가치함수를 업데이트 한다. 사람이 다음 순간을 지속적으로 예측하고 바로 학습하는 것처럼, 에이전트를 실시간으로 예측과 ground-truth 간의 차이로 학습시키기 위한 기법이다. 몬테카를로 예측에서 는 에피소드가 끝나야 그 값을 알 수 있었다. 시간차 예측에서는 비슷하지만 를 으로 나타낸 가치함수의 정의인 아래의 식을 이용한다. 다이내믹 프로그래밍처럼 기댓값을 계산하지는 않고 값을 샘플링해서 그 값으로 현재의 가치함수를 업데이트 한다. 가치함수의 업데이트는 실시간으로 이뤄지며, 몬테카를로 예측과는 달리 한번에 하나의 가치함수만 업데이트 한다. 에이전트는 현재 가지고 있는 가치함수 리스트에서 다음 상태에 해당하는 가치함수 을 가져올 수 있을 것이다. 그러면 바로 를 계산할 수 있고, 계산된 값은 의 가치함수 업데이트의 목표가 된다. 는 시간차 에러(Temporal-Difference Error)라고 하며 시간차 예측에서 업데이트의 목표는 반환값 과는 달리 실제의 값은 아니다. 은 현재 에이전트가 가지고 있는 값이고, 에이전트는 이 값을 의 가치함수일 것이라고 예측 할 뿐이다. 다른 상태의 가치함수 예측값을 통해 지금 상태의 가치함수를 예측하는 이러한 방식을 부트스트랩(Bootstrap)이라고 한다. 즉, 업데이트 목표도 정확하지 않은 상황에서 가치함수를 업데이트 하는 것이다. 시간차 예측은 에피소드가 끝날 때까지 기다릴 필요없이 가치함수를 바로 업데이트 할 수 있다. 하지만 이렇게 업데이트를 해도 몬테카를로 예측과 같이 원래 가치함수 값에 수렴할까? 충분히 많은 샘플링을 통해 업데이트 하면 참 가치함수에 수렴하며 많은 경우 몬테카를로 예측보다 더 효율적으로 빠른 시간안에 참 가치함수에 근접한다 고 한다. (이 부분에 대한 디테일한 설명은 나와있지 않다..😭) 하지만 시간차 예측은 몬테 카를로 예측보다 초기 가치함수 값에 따라 예측 정확도가 많이 달라진다는 단점 이 존재한다. 다음 포스트에서는 강화학습 알고리즘인 살사(SARSA)와 큐러닝(Q-learning)을 살펴볼 것이다!","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"MonteCarloApproximation","slug":"MonteCarloApproximation","permalink":"https://longshiine.github.io/tags/MonteCarloApproximation/"},{"name":"TemporalDifference","slug":"TemporalDifference","permalink":"https://longshiine.github.io/tags/TemporalDifference/"}]},{"title":"강화학습 기초: 정책 이터레이션, 가치 이터레이션","slug":"2021/01/24/강화학습-기초4","date":"2021-01-24T04:00:23.000Z","updated":"2023-01-06T05:28:22.586Z","comments":true,"path":"2021/01/24/강화학습-기초4/","link":"","permalink":"https://longshiine.github.io/2021/01/24/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EC%B4%884/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 다섯번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍정책 이터레이션 (Policy Iteration)결국 MDP로 정의되는 문제에서 알고 싶은 것은 가장 높은 보상을 얻게 하는 정책 을 찾는 것이다. 하지만 처음에는 이 정책을 알 수가 없다. 보통 처음에는 무작위로 행동을 정하는 정책으로부터 시작하여 계속 발전시켜 나간다. 하지만 우리가 얻고자 하는 최적 정책은 무작위정책(random policy)이 아니다. 그렇다면 현재의 정책을 평가 하고 더 나은 정책으로 발전 해야한다. 어떤 정책이 있을 때 정책평가(Policy Evaluation)를 통해 얼마나 좋은지 평가하고, 그 평가를 기준으로 정책발전(Policy Improvement)을 통해 좀 더 나은 정책으로 발전 시킨다. 이러한 과정을 무한히 반복하면 정책은 최적 정책으로 수렴 한다. 안선생의 안소리🤬 예상 : 무작위 정책에서 출발해서 도대체 어떻게 최적 정책에 수렴하지요? 자세히 설명해보세요!–&gt; https://www.youtube.com/watch?v=rrTxOkbHj-M&amp;t=19s 팡요랩 영상의 18분 즈음부터 참고(정리해보자면, 주위의 상태의 값들은 쓰레기 값이지만 보상 과 같은 작은 정보들을 토대로 점진적으로 업데이트 되어 최적에 다가가게 되는 것이다. 정수기에서 물이 정수되는 것처럼 말이다! 사실 아직도 업데이트가 되는것이 신기하다) 정책 평가 (Policy Evaluation)가치함수는 정책이 얼마나 좋은지 판단하는 근거가 된다. 가치함수는 현재 정책 를 따라갔을 때 받을 보상에 대한 기댓값⭐️ 이다. 에이전트의 목표는 어떻게 하면 보상을 많이 받을 수 있을지를 알아내는 것이므로 현재 정책에 따라 받을 보상에 대한 정보가 정책의 가치가 되는 것이다. $$ v\\pi (s) = E[R{t+1}+\\gamma G_{t+1}|S_t=s]$$ 정책 이터레이션에서는 위의 벨만 기대 방정식을 사용하여 문제를 풀 것이다. 핵심은 주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산 하겠다는 것이다. 이 과정은 한 타임스텝의 보상만 고려하고 주변 상태의 가치함수들이 참 가치함수들이 아니기 때문에 이렇게 계산해도 이 값은 실제값이 아니다. 하지만 이러한 계산을 여러번 반복 한다면 참 값으로 수렴 한다는 것이 메인 아이디어이다. 이전 포스트에서 위의 식을 그리드월드 예제에서 계산 가능한 형태로 변환해 보았었다. 아래는 k번째 가치함수를 통해 k+1번째 가치함수를 계산하는 식이다. 우리는 아래의 식을 반복적으로 계산해 나갈 것이다. 한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다. 번째 가치함수 matrix에서 현재 상태 에서 갈 수 있는 다음상태 에 저장돼 있는 가치함수 을 불러온다.(보라색 부분중의 하나) 에 할인율 를 곱하고 그 상태로 가는 행동에 대한 보상 을 더한다. 2번에서 구한 값에 그 행동을 할 확률, 즉 정책 값을 곱한다. 3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다. 4번 과정을 통해 더한 값을 번째 가치함수 matrix의 상태 자리에 저장한다. 1-5 과정을 모든 에 대해 반복한다. 이것은 한번의 정책평가 과정이다. 하지만 한번의 정책평가로서는 제대로 평가를 할 수 없어, 이과정을 여러번 반복하는데, 으로 시작해서 무한히 반복하면 참 가 될 수 있다. 정책 발전 (Policy Improvement)애초에 정책을 발전 시키지 않는다면 정책에 대한 평가는 의미가 없다. 그렇다면 정책 평가를 바탕으로 어떻게 정책을 발전시킬 수 있을까? 사실 정책 발전의 방법이 정해져있는 것은 아니다. 하지만 이 책에서는 가장 널리 알려진 탐욕 정책 발전(Greedy Policy Improvement)을 사용한다. 에이전트가 해야할 일은 단순하다. 상태 s에서 선택 가능한 행동의 즉 큐함수 값을 비교하고 그중에서 가장 큰 값을 가지는 행동을 선택하면 된다. 이것을 탐욕 정책 발전 이라고 하는데, 눈앞에 보이는 것 중에서 당장에 가장 큰 이익을 추구하는 것과 같은 모습 이기 때문에 이러한 이름이 붙었다. 탐욕 정책 발전을 통해 업데이트된 정책은 아래와 같다. max 함수와는 다르게 반환되는 것이 행동 이다. 탐욕 정책 발전을 통해 정책을 업데이트하면 이전 가치함수에 비해 업데이트된 정책으로 움직였을 때 받을 가치함수가 무조건 크거나 같다. 다이타믹 프로그래밍에서는 이처럼 탐욕 정책 발전을 사용하여 가장 큰값의 가치함수를 가지는 최적 정책에 수렴할 수 있다. 정책 이터레이션 코드우선 에이전트가 해야할 역할을 고려해서 전체 코드의 흐름을 보면 다음과 같다. 1234567891011121314151617181920212223class PolicyIteration: def __init__(self, env): # 환경에 대한 객체 self.env = env # 정책 평가 def policy_evaluation(self): pass # 정책 발전 def policy_improvement(self): pass # 특정 상태에서 정책에 따른 행동 def get_action(self, state): return actionif __name__ == \"__main__\": env = Env() policy_iteration = PolicyIteration(env) grid_world = GraphicDisplay(policy_iteration) grid_world.mainloop() 에이전트가 알고 있는 환경(env)의 정보는 다음과 같다. env.width, env.height: 그리드월드의 너비와 높이 env.state_after_aciton(state, action): 특정 상태에서 특정 행동을 했을 때 에이전트가 가는 다음 상태 env.get_all_states(): 존재하는 모든 상태 env.get_reward(state, action): 특정 상태의 보상 env.possible_actions: 상,하,좌,우 정책 이터레이션은 정책 평가 와 정책 발전 으로 이뤄져 있다. 따라서 각각의 함수를 정의 한다. policy_evaluation정책 평가를 통해 에이전트는 모든 상태의 가치함수를 업데이트 한다. 모든 상태에 대해 벨반 기대 방정식 의 계산이 끝나면 현재의 value_table에 next_value_table을 덮어쓰는 식으로 policy_evalutation을 진행한다. 정책 평가에 사용되는 벨만 기대 방정식이다. 이때, 상태 변환 확률 을 1이라고 설정 했기 때문에, 다음상태 은 만약 행동이 왼쪽일 경우 왼쪽에 있는 상태가 된다. 12345678910111213141516171819202122232425# 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가def policy_evaluation(self): # 다음 가치함수 초기화 next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)] # 모든 상태에 대해서 벨만 기대방정식을 계산 for state in self.env.get_all_states(): value = 0.0 # 마침 상태의 가치 함수 = 0 if state == [2, 2]: next_value_table[state[0]][state[1]] = value continue # 벨만 기대 방정식 for action in self.env.possible_actions: next_state = self.env.state_after_action(state, action) reward = self.env.get_reward(state, action) next_value = self.get_value(next_state) value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value)) next_value_table[state[0]][state[1]] = value self.value_table = next_value_table : for action in self.env.possible_actions: : self.get_policy(state)[action] : reward = self.env.get_reward(state, action) : self.discount_factor (0.9) : next_state = self.env.state_after_action(state, action) : self.get_value(next_state) : next_value_table[state[0]][state[1]] get_policy 함수를 통해 각 상태에서 각 행동에 대한 확률값을 구한다. 그리고 다음 상태로 갔을 때 받을 보상과 다음상태의 가치함수를 할인율을 적용하여 더한다. 정책이 각 행동에 대한 확률을 나타내기 때문에 모든 행동에 대해 value를 계산하고 더하면 기댓값을 계산한 것이 된다. policy_improvement정책 평가를 통해 정책을 평가하면 그에 따른 새로운 가치함수를 얻는다. 에이전트는 이제 새로운 가치함수를 통해 정책을 업데이트 해야한다. 정책발전에는 탐욕 정책 발전을 사용한다. 123456789101112131415161718192021222324252627282930# 현재 가치 함수에 대해서 탐욕 정책 발전def policy_improvement(self): next_policy = self.policy_table for state in self.env.get_all_states(): if state == [2, 2]: continue value_list = [] # 반환할 정책 초기화 result = [0.0, 0.0, 0.0, 0.0] # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산 for index, action in enumerate(self.env.possible_actions): next_state = self.env.state_after_action(state, action) reward = self.env.get_reward(state, action) next_value = self.get_value(next_state) value = reward + self.discount_factor * next_value value_list.append(value) # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전 max_idx_list = np.argwhere(value_list == np.amax(value_list)) max_idx_list = max_idx_list.flatten().tolist() prob = 1 / len(max_idx_list) for idx in max_idx_list: result[idx] = prob next_policy[state[0]][state[1]] = result self.policy_table = next_policy 탐욕 정책 발전은 가치가 가장 높은 하나의 행동을 선택하는 것이다. 하지만 이 예제에서와 같이 현재 상태에서 가장 좋은 행동이 여러개일 수도 있다. 그럴때에는 가장 좋은 행동들을 동일한 확률로 선택하는 정책으로 업데이트한다. 현재 상태에서 가능한 행동에 대해 을 계산한다. 계산한 값을 value_list에 저장한다. max 함수를 통해 value_list에 담긴 값 중 가장 큰 값을 알아낸다. argwhere를 통해 가장 큰 값의 index를 알아내고(여러개면 여러개 반환) max_idx_list에 저장한다. max_idx_list의 길이를 바탕으로 확률을 계산하여 확률값을 계산하고 result에 저장한다. 이렇게 되면 policy_table에는 업데이트 된 정책(각 상태의 행동에 대한 확률)이 저장된다. 정책 이터레이션에서 에이전트는 정책 평가와 정책 발전을 반복하여 최적 정책을 찾아낸다. 가치 이터레이션정책 이터레이션과 가치 이터레이션의 중요한 차이점은 정책 이터레이션에서는 정책 평가와 정책 발전 으로 단계가 나누어져 있다면 가치 이터레이션에서는 그렇지 않다는 것이다. 가치 이터레이션은 현재의 가치함수가 최적 정책에 대한 가치함수라고 가정하기 때문에 정책을 발전하는 함수가 따로 필요하지 않다. 따라서 최적 행동을 반환하는 get_action 함수를 정책을 출력하는 데 대신 사용한다. 주요 코드의 전체적인 흐름은 다음과 같다. 123456789101112131415161718192021class ValueIteration: def __init__(self, env): # 환경 객체 생성 self.env = env # 벨만 최적 방정식을 통해 다음 가치함수 계산 def value_iteration(self): return # 현재 가치함수로부터 행동을 반환 def get_action(self, state): return def get_value(self, state): returnif __name__ == \"__main__\": env = Env() value_iteration = ValueIteration(env) grid_world = GraphicDisplay(value_iteration) gird_world.mainloop() 정책 이터레이션에서는 policy_evaluation 함수에서 벨만 기대 방정식을 통해 다음 가치함수를 계산했다. 가치 이터레이션에서는 value_iteration 함수를 통해 다음 가치함수를 계산한다. 12345678910111213141516171819202122232425# 벨만 최적 방정식을 통해 다음 가치 함수 계산def value_iteration(self): # 다음 가치함수 초기화 next_value_table = [[0.0] * self.env.width for _ in range(self.env.height)] # 모든 상태에 대해서 벨만 최적방정식을 계산 for state in self.env.get_all_states(): # 마침 상태의 가치 함수 = 0 if state == [2, 2]: next_value_table[state[0]][state[1]] = 0.0 continue # 벨만 최적 방정식 value_list = [] for action in self.env.possible_actions: next_state = self.env.state_after_action(state, action) reward = self.env.get_reward(state, action) next_value = self.get_value(next_state) value_list.append((reward + self.discount_factor * next_value)) # 최댓값을 다음 가치 함수로 대입 next_value_table[state[0]][state[1]] = max(value_list) self.value_table = next_value_table 위의 벨만 최적 방정식을 계산하여 value_list에 저장한다. value_list에 저장된 값중 최대의 값을 새로운 가치함수로 저장한다. 123456789101112131415161718# 현재 가치 함수로부터 행동을 반환def get_action(self, state): if state == [2, 2]: return [] # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산 value_list = [] for action in self.env.possible_actions: next_state = self.env.state_after_action(state, action) reward = self.env.get_reward(state, action) next_value = self.get_value(next_state) value = (reward + self.discount_factor * next_value) value_list.append(value) # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환 max_idx_list = np.argwhere(value_list == np.amax(value_list)) action_list = max_idx_list.flatten().tolist() return action_list 모든 행동에 대해 큐함수를 구한다. 그 중 가장 큰 value 값을 가지는 행동의 인덱스를 가져온다(여러개라면 여러개 모두 가져온다) 행동들을 모두 action_list에 저장한다. 이 예제에 대해서는 Calculate 6번 정도에 가치함수가 거의 수렴하게 된다. 이때 수렴한 가치함수의 값을 토대로 정책을 출력해보면 아래의 그림과 같다. 다이내믹 프로그래밍의 한계와 강화학습벨만 방정식을 이용한 다이내믹 프로그래밍으로서 정책 이터레이션과 가치 이터레이션을 살펴봤다. 하지만 다이내믹 프로그래밍은 계산을 빠르게 하는 것이지 학습을 하는 것 은 아니다. 그렇다면 이러한 다이내믹 프로그래밍의 한계는 무엇일까? 1. 계산복잡도 : 문제의 규모가 커지만 계산으로 푸는데에 한계. 계산복잡도 = 상태 크기의 3제곱2. 차원의 저주 : 그리드월드의 상태의 차원은 2차원. 상태의 차원이 늘어나면 상태의 수가 지수적으로 증가3. 환경에 대한 완벽한 정보가 필요 : 보상과 상태 변환 확률을 정확히 안다는 가정 필요. 보통은 이 정보를 확실히 알 수 없음. 현실세계의 환경에 놓인 문제를 풀어내는 데에는 위의 세 가지 한계가 치명적으로 작용한다. 때문에 이러한 한계를 극복하기 위해 환경을 모르지만 환경과의 상호작용을 통해 경험을 바탕으로 학습하는 방법 이 등장한다. 바로 강화학습이다. 모델없이 학습하는 강화학습환경의 모델이란 무엇일까? MDP에서 환경의 모델은 상태 변환 확률과 보상이다. 현재 이 책에서 다루고 싶은 모델은 수학적 모델 로서 시스템에 입력이 들어왔을 때 시스템이 어떤 출력을 내는지에 대한 방정식이다. 이처럼 입력과 출력의 관계를 식으로 나타내는 과정을 모델링(Modeling)이라고 한다. 사실 입력과 출력 사이의 방정식은 정확할 수가 없다. 방정식에서는 A라는 입력이 들어와서 B라는 출력이 나오더라도 실제 세상에서는 B라는 출력이 절대로 나오지 않는다. 모델은 정확하면 정확할수록 복잡하며 공기나 바람같은 자연현상을 정확하게 모델링하는 것은 불가능 에 가깝다. 게임에서는 사실 사람이 환경을 만들었고, 사람이 정해준대로만 게임이 작동하므로 모델링 오차는 없다 고 볼 수 있다. 하지만 게임을 벗어난다면 글쎄…🤔 모델을 정확히 알기 어려운 경우, 시스템의 입력과 출력 사이의 관계를 알기 위해 두가지 접근 방법으로 접근해 볼 수 있다. 할 수 있는 선에서 정확한 모델링을 한 다음, 모델링 오차에 대한 부분을 실험을 통해 조정한다. 모델 없이 환경과의 상호작용을 통해 입력과 출력 사이의 관계를 학습한다. 1번은 학습의 개념없이 고전적으로 많이 적용하는 방법이며 시스템의 안정성을 보장한다. 하지만 문제가 복잡해지고 어려워질수록 한계가 있다. 2번은 학습의 개념이 들어간다. 학습의 특성상 모든 상황에서 동일하게 작동한다고 보장할 수 없지만 많은 복잡한 문제에서 모델이 필요없는 것은 장점이다. 2번 방법이 바로 이 책의 주제인 강화학습 이다. 정리 정책 이터레이션: 벨만 기대 방정식을 이용해 정책을 평가하고, 탐욕 정책 발전을 이용해 정책 발전 가치 이터레이션: 최적 정책을 가정하고 벨만 최적 방정식 이용. 정책이 직접적으로 주어지지 않아 큐함수를 통해 행동 선택 다이내믹 프로그래밍의 한계: 계산 복잡도, 차원의 저주, 환경에 대한 완벽한 정보 필요 3장 한줄평 짧고 간결하게 쓰는게 오히려 힘들다..","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"PolicyIteration","slug":"PolicyIteration","permalink":"https://longshiine.github.io/tags/PolicyIteration/"},{"name":"ValueIteration","slug":"ValueIteration","permalink":"https://longshiine.github.io/tags/ValueIteration/"}]},{"title":"강화학습 기초: 그리드월드와 다이내믹 프로그래밍","slug":"2021/01/23/강화학습-기초3","date":"2021-01-23T03:58:43.000Z","updated":"2023-01-06T05:25:23.575Z","comments":true,"path":"2021/01/23/강화학습-기초3/","link":"","permalink":"https://longshiine.github.io/2021/01/23/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EC%B4%883/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 네번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍 지금까지 열심히 강화학습 문제란 무엇인지, 문제를 어떻게 수학적으로 정의할 수 있는지, 그러한 문제의 최적의 방정식은 어떻게 구성되었는지를 살펴보았다. 이제 본격적으로 예제와 함께 문제를 풀어볼 시간이다 👉 다이내믹 프로그래밍(Dynamic Programming)은 작은 문제가 큰 문제 안에 중첩되어 있는 경우, 작은 문제의 답을 다른 작은 문제에서 이용함으로써 효율적으로 계산하는 방법이다. 다이내믹 프로그래밍을 이용하여, 벨만 기대 방정식 을 푸는 것 –&gt; 정책 이터레이션 벨만 최적 방정식 을 푸는 것 –&gt; 가치 이터레이션 이며, 이번 장에서는 정책 이터레이션과 가치 이터레이션을 그리드월드 예제를 통해 코드로 실습해본다. 다이내믹 프로그래밍은 이후에 강화학습의 근간이 되기 때문에 제대로 이해하는 것이 중요하다!! 순차적 행동 결정 문제강화학습 은 순차적으로 행동을 결정해야 하는 문제를 푸는 방법중 하나 이다. 2장에서 MDP 를 정의하고 벨만 방정식 을 세우는 과정을 다뤘다. 순차적 행동 결정 문제를 푸는 방법을 정리하면 아래와 같다. 순차적 행동문제 –&gt; MDP로 전환 가치함수를 벨만 방정식으로 반복적으로 계산 최적 가치함수와 최적 정책 도출 강화학습 또한 순차적 행동 결정 문제를 푸는 방법이기 때문에, 벨만 방정식을 이해해야 강화학습을 이해할 수 있다. 그렇다면 벨만 방정식을 푼다는 것 은 어떤 의미일까? 보통 수학에서 “방정식을 푼다” 라고 하면 식을 만족하는 변수의 값을 찾는 것 을 말한다. 벨만 방정식을 통해 에이전트가 하고 싶은 것은 아래의 식을 만족하는 을 찾는 것이다. 이 값을 찾는다면 벨만 방정식은 풀린 것이며 에이전트는 최적 가치함수를 알아낸 것이다. 다이내믹 프로그래밍다이내믹 프로그래밍의 기본적인 아이디어는 큰 문제 안에 작은 문제들이 중첩된 경우, 전체 큰 문제를 작은 문제로 쪼개서 풀겠다는 것이다. 이때 각각의 작은 문제들이 별개가 아니기 때문에 작은 문제들의 해답을 서로서로 이용할 수 있다. 이 특성을 이용하면 결과적으로 계산량을 줄일 수 있다. (책에는 꽤 길게 서술되어 있는데 조금 생략했다) 문제의 목표는 각 상태의 참 가치함수를 구하는 것이다. 즉, 의 참값을 구하는 것이다. 이 큰 문제를 작은 문제로 나누어서 구하게 되면, 아래와 같이 풀 수 있다. 한번의 화살표는 한 번의 계산으로서 에서 이 되는 과정이다. 이 계산은 모든 상태에 대해 진행하며 한번 계산이 끝나면 모든 상태의 가치함수를 업데이트한다. 다음 계산은 업데이트된 가치함수를 이용해 다시 똑같은 과정을 반복하는 것이다. 이런 식으로 계산하면 이전의 정보를 이용해 효율적으로 업데이트할 수 있게 된다. 격자로 이뤄진 간단한 예제: 그리드월드 예시로 우리가 풀어볼 문제는 다음과 같다. 빨간색 네모는 에이전트를 의미하고, 에이전트는 파란색 동그라미로 가야한다. 이때 (-1)의 보상을 주는 연두색 세모가 막고 있어, 세모를 피해서 네모에 도착해 (+1)의 보상을 받는 것이다. 파란색에 도착하는 최적 정책 을 찾는 것이 우리의 목표다. 강화학습 알고리즘의 흐름MDP부터 강화학습의 기본적인 알고리즘까지 전반적인 흐름을 도식으로 보면 다음과 같다. 자, 순차적 행동 결정 문제는 MDP를 통해 정의되었고, MDP로 정의되는 목표는 에이전트가 받을 보상의 합을 최대 로 하는 것 이며, 이는 벨만 방정식을 품으로써 달성할 수 있다. 또한 벨만 방정식은 다이내믹 프로그래밍을 통해 풀 수 있으며, 다이내믹 프로그래밍에는 정책 이터레이션(policy iteration)과 가치 이터레이션(value iteration)이 있다. 이 두 방법은 후에 살사(SARSA)로 발전하며, 살사는 큐러닝(Q-Learning)으로 이어진다. 이제 실제 코드를 통해 에이전트를 학습시켜보자!","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"DynamicProgramming","slug":"DynamicProgramming","permalink":"https://longshiine.github.io/tags/DynamicProgramming/"},{"name":"GridWorld","slug":"GridWorld","permalink":"https://longshiine.github.io/tags/GridWorld/"}]},{"title":"강화학습 기초: 가치함수와 벨만방정식","slug":"2021/01/22/강화학습-기초2","date":"2021-01-22T07:12:02.000Z","updated":"2023-01-06T05:25:07.791Z","comments":true,"path":"2021/01/22/강화학습-기초2/","link":"","permalink":"https://longshiine.github.io/2021/01/22/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EC%B4%882/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 2장 강화학습 기초 1: 가치함수와 벨만 방정식 이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의 했다. 에이전트는 MDP를 통해 최적 정책 을 찾으면 된다. 이제 에이전트가 어떻게 최적 정책을 찾을 수 있을지 보다 구체적으로 알아보도록 하자!(❗️수식 주의) 가치함수에이전트의 입장에서 어떤 행동을 하는 것이 좋은지는 어떻게 알 수 있을까? 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할텐데, 아직 받지 않는 보상들을 어떻게 고려한단 말일까?🧐 앞으로 받을 보상에 관련된 개념 이 바로 가치 함수이다. 현재 시간 t로부터 에이전트가 행동을 하면서 받을 보상들을 단순히 합한다는 생각을 해볼 수 있다. 그렇게 되면 $R_{t+1}+R_{t+2}+R_{t+3}+R_{t+4}+R_{t+5}+…$와 같은 꼴일 것이다. 그런데 시간에 따른 보상을 이렇게 단순하게 더한다면 세가지 문제가 생긴다. 지금 받은 보상이나 미래에 받는 보상이나 똑같이 취급한다. 100이라는 보상을 1번 받는 것과 20이라는 보상을 5번 받는 것을 구분할 방법이 없다. 시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대이고 1씩 받아도 합이 무한대이다. 이러한 문제 때문에 에이전트는 단순한 보상의 합으로는 시간 t에 있었던 상태가 어떤 가치를 가지는지 판단하기가 어렵다. 따라서 좀 더 정확하게 상태의 가치를 판단하기 위해 이전 포스트에서 설명한 할인율이라는 개념을 사용한다. 할인율을 적용하여 시간 t 이후의 시간 t시점에서의 보상 을 모두 더한 것을 반환값이라고 하며 $G_t$로 표현한다. 반환값을 수식으로 표현하면 아래와 같다. $$G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+\\gamma^4 R_{t+5}…$$ 반환값이라는 것은 에이전트가 실제로 환경을 탐험하며 받은 보상의 합으로, 이 책에서는 에이전트와 환경이 유한한 시간동안 상호작용 하는 경우만 다룬다. 이렇게 유한한 에이전트와 환경의 상호작용 을 에피소드라고 부른다. 에피소드 에서는 에피소드를 끝낼 수 있는 마지막 상태가 있는데, 체스의 경우를 생각한다면 킹을 잃는 순간이 마지막 상태가 된다. 에이전트가 에피소드가 끝난 후에 ‘그때로부터 얼마의 보상을 받았지?’ 라며 보상을 정산하는 것이 반환값이다. 만일 에피소드를 t=1 부터 5까지 진행했다면 에피소드가 끝난 후에 방문했던 상태들에 대한 5개의 반환값이 생길 것이다. $$G_1 = R_{2}+\\gamma R_{3}+\\gamma^2 R_{4}+\\gamma^3 R_{5}+\\gamma^4 R_{6}\\\\G_2 = R_{3}+\\gamma R_{4}+\\gamma^2 R_{5}+\\gamma^3 R_{6}\\\\G_3 = R_{4}+\\gamma R_{5}+\\gamma^2 R_{6}\\\\G_4 = R_{5}+\\gamma R_{6}\\\\G_5 = R_{6}$$ MDP로 정의 되는 세계에서 반환값은 에피소드마다 다를 수 있다. 때문에 에이전트는 특정 상태의 가치를 반환값에 대한 기댓값 으로 판단해야 한다. 이것이 바로 가치함수의 개념이다. 따라서 가치함수는 아래와 같이 표현된다. $$v(s) = E[G_t|S_t=s]$$ 각 타임스텝마다 받는 보상이 모두 확률적이고 반환값이 그 보상들의 합이므로 반환값은 확률변수 이다. 하지만 가치함수는 확률변수가 아니라 특정 양을 나타내는 값이므로 소문자로 표현한다. 가치함수의 식에 위에서 정의한 반환값의 수식을 대입하고 정리해보면 아래와 같다. $$v(s) = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}…|S_t=s]\\\\ = E[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}…)|S_t=s]\\\\ = E[R_{t+1}+\\gamma G_{t+1}|S_t=s]$$ 사실 $R_{t+2}+\\gamma R_{t+3} …$ 부분을 반환값의 형태로 표현하긴 했지만 사실 에이전트가 실제로 받은 보상이 아니다. 이 보상은 앞으로 받을 것이라 예상하는 보상 으로써 이 부분 또한 가치함수로 표현할 수 있다. $$v(s) = E[R_{t+1}+\\gamma v(S_{t+1})|S_t=s]$$ 여기까지는 가치함수를 정의할 때 정책을 고려하지 않았다. 하지만 에이전트가 앞으로 받을 보상에 대해 생각할 때 정책을 고려하지 않으면 안된다. 왜냐하면 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야 하고 각 상태에서의 행동을 결정하는 것 이 에이전트의 정책이기 때문이다. 보상은 어떤 상태에서 어떤 행동을 하는지에 따라 환경에서 에이전트에게 주어진다. 이말은 곧 MDP로 정의 되는 문제에서의 가치함수가 행동을 결정하는 정책에 의존한다는 것이다. 따라서 아래의 수식처럼 가치함수에 아래 첨자로 정책을 쓰면 더 명확한 수식이 된다. $$v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]$$ 이 수식이 바로 강화학습에서 상당히 중요한 벨만 기대 방정식(Bellman Expectation Equation)이다.벨만 기대 방정식은 현재 상태의 가치함수 $v_\\pi (s)$와 다음상태의 가치함수 $v_\\pi (S_{t+1})$ 사이의 관계 를 말해주는 방정식이다. 강화학습은 벨만 방정식을 어떻게 풀어가느냐의 스토리이다 🤧 큐함수가치함수는 말 그대로 “함수” 이다. 따라서 입력이 무엇이고 출력이 무엇인지 알 필요가 있다. 지금까지 설명한 가치함수는 상태 가치함수(state value-function)으로써 상태 가 입력으로 들어오면 그 상태에서 앞으로 받을 보상의 합 을 출력하는 함수이다. 따라서 에이전트는 가치함수를 통해 어떤 상태에 있는 것이 얼마나 좋은지를 알 수 있다. 상태 가치함수가 각 상태에 대해 가치를 알려주는 것처럼 각 행동에 대해 가치를 알려주는 함수가 있다면 어떨까? 아마 에이전트는 그 함수의 값만 보고 바로 행동을 선택할 수 있을 것이다. 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수를 큐함수(Q Function), 다른말로 행동 가치함수 라고 한다. 큐함수는 상태, 행동이라는 두가지 변수를 가지며 $q_\\pi (s,a)$라고 나타낸다. 또한 가치함수와 큐함수 사이의 관계식은 다음과 같이 표현할 수 있다. $$v_\\pi (s) = \\sum_{a \\in A} \\pi (a|s) q_\\pi (s,a)$$ 각 행동을 했을 때 앞으로 받을 보상인 큐함수 $q_\\pi (s,a)$를 정책 $\\pi (a|s)$에 곱한다 모든 행동에 대해 큐함수와 $\\pi (a|s)$를 곱한 값을 더하면 가치함수가 된다. 큐함수는 강화학습에서 중요한 역할을 한다. 강화학습에서 에이전트가 행동을 선택하는 기준 으로 가치 함수 보다는 보통 큐함수 를 사용한다. 그 이유는 뒤에서 등장한다! 큐함수 또한 벨만 기대 방정식의 형태로 나타 낼수 있으며 조건문에 행동이 들어간다는 점에서 가치함수의 식과 다르다. $$q_\\pi (s,a) = E_\\pi [R_{t+1}+ \\gamma q_\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]$$ 벨만 기대 방정식이제 2장의 메인 디쉬인 벨만 기대 방정식을 찬찬히 살펴보자(👂🏻집중). 살짝 정리를 해보면 벨만 기대 방정식이라고 하는 이유는 식에 기댓값의 개념 이 들어가기 때문이고, 이 벨만 방정식은 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계 를 식으로 나타낸 것이었다. 벨만 방정식은 강화학습에서 상당히 중요한 부분을 차지한다. 벨만 방정식이 강화학습에서 왜 그렇게 중요한 위치를 차지하고 있는 걸까?🤔 앞에서 정의했던 가치함수의 정의를 다시 한번 살펴보자. $$v_\\pi (s) = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}…|S_t=s]$$ 이 수식으로 부터 기댓값을 알아내려면 앞으로 받을 보상에 대해 고려해야하고, 이는 정의상으로는 가능하지만 상태가 많아질수록 상당히 비효율적인 방법 이다. 따라서 컴퓨터가 이 기댓값을 계산하기 위해 다른 조치가 필요하다. 예를 들어 1을 100번 더해야하는 문제가 있다고 해보자. 식 하나로 풀어내는 방법은 아래와 같다 $$1+1+1+…+1 = 100$$하지만 다른 방법으로 접근해볼 수도 있다. x라는 변수를 지정해 그 값에 1을 계속 더해나가는 것이다. 123X = 0for i in range(100): X = X + 1 벨만 방정식으로 가치함수를 계산하는 것은 두 번째 방식과 같은 것이다. 한 번에 모든 것을 계산하는 것이 아니라 값을 변수에 저장하고, 루프를 도는 계산을 통해 참 값을 알아나가는 것이다. 즉, 현재 가치함수 값을 업데이트 해 나가는 것이다. 하지만 업데이트 하려면 기댓값을 계산해야 하는데 기댓값은 어떻게 계산할 수 있을까? 기댓값에는 어떠한 행동을 할 확률(정책 $\\pi (a|s)$)과 그 행동을 했을 때 어떤 상태로 가게 되는 확률(상태 변환 확률 $P_{ss’}^a$)이 포함되어 있다. 따라서 정책과 상태 변환 확률을 포함해서 계산하면 된다. $$v_\\pi (s) = \\sum_{a \\in A} \\pi (a|s) (r_{(s,a)} + \\gamma \\sum_{s’ \\in S} P_{ss’}^a v_\\pi (s’) )$$ 예시를 한번 살펴보는 것이 매우 도움이 될 것이다. 설명을 용이하게 하기 위해, 상태 변환 확률($P_{ss’}^a$)은 모두 1이라고 생각하고(왼쪽으로 가기로 결정했다면 1의 확률로 왼쪽으로 간다고 하자) 정책($\\pi (a|s)$)은 무작위로 행동하는 것으로서 각 행동이 25%의 확률로 선택이 되며, 할인율($\\gamma$)은 0.9라고 생각하고, $s’$은 모든 상태일 수 있으나, 에이전트의 행동으로 도착하게 될 상태라고 해보자. 회색 별은 $r(s,a)$에 해당하는 보상 값이다. 위의 가정을 적용하여 식을 바꾸어 본다면, $$ v\\pi (s) = \\sum{a \\in A} 0.25 (r*{(s,a)} + (0.9)v*\\pi (s’) ) $$ 와 같이 바꿀 수 있고, 위의 표와 같이 계산을 할 수 있게 된다. 벨만 기대 방정식을 이용해 현재의 가치함수를 계속 업데이트 하다 보면,에이전트가 얻을 실제 보상에 대한 참 기댓값을 얻을 수 있다. 벨만 최적 방정식처음의 가치함수의 값들은 의미가 없는 값으로 초기화 된다. 초기값으로 부터 시작해서 벨만 기대 방정식으로 반복적으로 계산한다고 가정해보자. 이 계산을 반복하다보면 방정식의 우항과 좌항이 같아진다(무한히 반복한다는 가정하에). 즉, $v_\\pi (s)$값이 수렴하는 것이다. 그렇다면 현재의 정책 $\\pi$에 대한 참 가치함수 를 구한것이다. $$v_{k+1}(s) \\leftarrow \\sum_{a \\in A} \\pi (a|s)(r_{(s,a)}+ \\gamma v_k(s’))$$ 벨만 기대 방정식을 기댓값을 계산하기 위해 살짝 변형하면, 현재 정책에 대한 참 가치함수 를 구할 수 있다. 위의 식은 뒤에서 배울 dynamic programming에서 자세히 다루도록 한다. 그러나 참 가치함수 와 최적 가치함수(Optimal Value Function) 은 다르다⭐️. 참 가치함수는 어떤 정책을 따라서 움직였을 경우에 받게되는 보상에 대한 참값 이고, 최적의 가치함수는 수많은 정책 중에서 가장 높은 보상을 얻게 되는 정책을 따랐을 때의 가치함수 이다. 그렇다. 우리는 단순히 현재 정책에 대한 가치함수를 구하고 싶은게 아닌, 최적 정책을 찾고 싶은 것이다. 단순히 현 정책에 대한 가치함수를 찾는 것이 아닌 더 좋은 정책으로 현재의 정책을 업데이트 해 나가야 한다. 이쯤에서 이런 질문이 들어야 한다. 더 좋은 정책이라는 것의 정의는 무엇인가? 어떤 정책이 더 좋은 정책이라고 판단할 수 있을까? 더 좋은 정책이란 정책을 따라갔을 때 받을 보상들의 합이 더 큰 경우라고 말할 수 있을 것이다. 그리고 그것은 가치함수 를 통해 판단할 수 있었다. 결국 가치함수가 정책이 얼마나 좋은지를 말해주는 것이다. 따라서 모든 정책 중, 가장 큰 가치함수를 갖는 정책이 최적 정책이다. 최적 큐함수 또한 같은 방식으로 생각한다. $$v^*(s) = max_{\\pi}[v_\\pi (s)]\\\\q^*(s,a) = max_{\\pi}[q_\\pi (s,a)]$$ 가장 높은 가치함수(큐함수)를 에이전트가 찾았다고 가정해보자. 이때 최적 정책 은 각 상태 s에서의 최적의 큐함수에 대해 가장 큰 값을 가진 행동을 하는 것이다. 즉, 선택 상황에서 판단 기준은 큐함수이며, 최적 정책은 최적 큐함수 $q^*$만 안다면 아래와 같이 구할 수 있다. $$\\pi^*(s,a) = \\begin{cases} \\displaystyle 1, \\;if \\; a = argmax_{a \\in A} \\;q^*(s,a) \\\\ \\displaystyle 0, \\;otherwise\\end{cases}$$ 그렇다면 최적의 큐함수는 어떻게 구할 수 있을까?그것을 구하는 것이 순차적 행동 결정 문제(MDP) 를 푸는 것이다. 어떻게 최적의 가치함수를 구하는지에 대해서는 다음 장에서 다룬다. 여기서는 최적의 가치함수 끼리 관계가 어떻게 되는지를 살펴보자. 현재 상태의 가치함수가 최적이라고 가정해보자. 현재상태의 가치함수가 최적이라는 것은 에이전트가 가장 좋은 행동을 선택한다는 것이다. 이때 선택의 기준이 되는 큐함수는 최적의 큐함수 이어야 하고, 따라서 다음이 최적의 가치함수의 식이 된다.$$v^*(s) = max_{a}[q^*(s,a) | S_t=s, A_t=a]$$ 여기서 큐함수를 가치함수로 고쳐서 표현하면 아래와 같다. $$v^*(s) = max_{a}E[R_{t+1} + \\gamma v^*(S_{t+1}) | S_t=s, A_t=a]$$ 바로 이 식을 벨만 최적 방정식(Bellman Optimality Equation) 이라 부르며, 이식은 최적의 가치함수에 대한 것이다. 큐함수에 대해서도 벨만 최적 방정식을 표현할 수 있는데, 아래와 같이 표현한다. $$q^*(s,a) = E[R_{t+1} + \\gamma max_{a’}q^*(S_{t+1}, a’) | S_t=s, A_t=a]$$ 기댓값인 이유는 다음 상태가 상태 변환 확률에 따라 달라지기 때문이다. 벨만 기대 방정식과 벨만 최적 방정식을 이용해 MDP로 정의되는 문제를 계산 으로 푸는 방법이 바로 다음장에서 다룰 다이내믹 프로그래밍(Dynamic programming)이다. 정리 MDP: 순차적 행동 결정 문제를 수학적으로 정의하는 것. 상태($S_t$), 행동($A_t$), 보상함수($r(s,a)$), 상태 변환 확률($P_{ss’}^a$), 할인율($\\gamma$)로 구성. 가치함수: 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합.$$v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]$$ 큐함수: 각 행동에 대해 가치를 알려주는 함수, 정책 업데이트 시에 사용$$q_\\pi (s,a) = E_\\pi [R_{t+1}+ \\gamma q_\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]$$ 벨만 기대 방정식: 현재 상태의 가치함수와 다음 상태 가치함수의 관계식$$v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]$$ 벨만 최적 방정식: 최적의 정책을 따르는 가치함수와 다음 상태 가치함수의 관계식$$v^*(s) = max_{a}E[R_{t+1} + \\gamma v^*(S_{t+1}) | S_t=s, A_t=a]$$ 2장 한줄평 최적 방정식 부분을 좀 더 명확하게 다시 정리해 봐야겠다. 🤥","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"BellmanEquation","slug":"BellmanEquation","permalink":"https://longshiine.github.io/tags/BellmanEquation/"},{"name":"ValueFunction","slug":"ValueFunction","permalink":"https://longshiine.github.io/tags/ValueFunction/"}]},{"title":"강화학습 기초: MDP(Markov Decision Process)","slug":"2021/01/20/강화학습-기초1","date":"2021-01-20T11:42:17.000Z","updated":"2023-01-06T05:25:16.469Z","comments":true,"path":"2021/01/20/강화학습-기초1/","link":"","permalink":"https://longshiine.github.io/2021/01/20/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B8%B0%EC%B4%881/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 두번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 2장 강화학습 기초 1: MDP(Markov Decision Process) MDP앞서 살펴보았듯 MDP는 순차적으로 결정해야하는 문제를 수학적으로 표현한다. 문제를 잘못 정의하면 에이전트가 학습을 못할 수도 있다. 따라서 문제의 정의는 에이전트가 학습하는 데 가장 중요한 단계 중 하나이다. 에이전트를 구현하는 사람은 학습하기에 많지도 않고 적지도 않은 적절한 정보를 에이전트가 알 수도 있도록 문제를 정의해야 한다. MDP의 이해를 돕기위해 이제 그리드 월드(Grid World)라는 예제를 살펴볼 것인데, 그리드는 격자이고 그리드 월드는 격자로 이뤄진 환경에서 문제를 푸는 각종 예제를 뜻한다. 어디한번 MDP의 구성요소를 하나하나 살펴보자. 상태$S$는 에이전트가 관찰 가능한 상태의 집합이다. 상태라는 말의 의미가 모호할 수 있는데 자신의 상황에 대한 관찰 이 상태에 대한 가장 정확한 표현이다. 로봇과 같은 실제 세상에서의 에이전트에게 상태는 센서 값이 될 것이다. 하지만 이 책에서와 같이 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해줘야 한다. 이때 ‘내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?’ 라는 질문을 스스로에게 던져보는 것이 좋다. 위의 그림과 같이 그리드월드(5x5)에서는 격자 상의 각 위치(좌표) 가 상태가 된다. 그리드월드(5x5)의 상태는 모든 격자의 위치로서 총 25개가 있다. 각 상태는 (x,y)로 이뤄진 좌표로서 그리드월드의 가로축이 x축이고 세로축이 y축이다. 식으로는 다음과 같이 표현할 수 있다. $$S = \\{ (1,1),(1,2),(1,3),…,(5,5)\\} $$ 에이전트는 시간에 따라 25개의 상태의 집합 안에 있는 상태를 탐험하게 된다. 시간은 t라고 표현하고, 시간 t일때의 상태를 $S_t$라고 표현하는데, 만약 시간이 t일때 상태가 (1,3)이라면 $S_t = (1,3)$와 같이 표현한다. MDP 에서 상태는 시간에 따라 확률적으로 변한다. t = 1일때 상태는 $S_t=(1,3)$일 수도 $S_t=(4,2)$일 수도 있다. 시간 t에서 에이전트가 있을 상태가 확률 변수 라는 뜻이다. 예를 들어 주사위를 던진다고 할때, 주사위를 던지는 실험은 임의 실험이고 주사위를 던져서 나오는 값은 변수가 된다. 임의의 실험에서 나오는 변수는 자신이 나타날 확률값을 가지고 있으며, 이 변수는 확률 변수가 된다. 보통 “시간 t에서의 상태 $S_t$가 어떤 상태 $s$다” 를 표현할때 $S_t = s$와 같이 적는다. 임의의 시간 t에서의 상태 =&gt; $S_t$ &gt; $S_t$가 어떤 상태 $s$다 =&gt; $S_t = s$ 행동에이전트가 상태 $S_t$에서 할 수 있는 가능한 행동의 집합은 $A$이다. 보통 에이전트가 할 수 있는 행동은 모든 상태에서 같다. 따라서 하나의 집합 $A$로 나타낼 수 있다. 시간 t에서의 행동 a는 $A_t = a$와 같이 표현하며, t라는 시간에 에이전트가 어떤 행동을 할지 정해져 있는 것이 아니므로 $A_t$와 같이 대문자로 표현한다. 즉 확률변수이다 그리드월드에서 에이전트가 할 수 있는 행동의 집합은 아래와 같다. $$ A = \\{up, down, left, right\\} $$ 만약 시간 t에서 $S_t=(3,1)$이고 $A_t=right$ 라면 $S_{t+1} = (4,1)$이다. 그런데 바람과 같은 예상치 못한 요소가 있다면 에이전트는 (4,1)에 도달하지 못할 수도 있고, 이러한 요소를 포함하여 에이전트가 어디로 이동할지 결정하는 것을 상태 변환 확률이라고 한다. 이는 조금 뒤 자세히 다루도록 하겠다. 보상함수보상(reward) 은 에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보이다. 시간 t에서 상태가 $S_t = s$이고 행동이 $A_t = a$일 때 보상함수는 아래와 같이 정의된다. $$r(s,a) = E[R_{t+1} | S_t=s, A_t=a] $$ 보상함수는 시간 t일 때 상태가 $S_t=s$이고 그 상태에서 행동 $A_t=a$를 했을 경우에 받을 보상에 대한 기댓값 $E$이다. 여기서 기댓값 은 무엇일까? (한번쯤 쉽게 풀어써보고 싶었는데 책의 예시가 아주 적절하다🤩) 기댓값 이란 일종의 평균이다. 주사위의 기댓값을 한번 생각해보면, 모든 면이 ${1 \\over 6}$의 동등한 확률로 나올테고, 주사위에서는 다음과 같은 값이 나올 것이라고 기대 할 수 있다. $$기댓값_{주사위} = 1*{1 \\over 6}+2*{1 \\over 6}+3*{1 \\over 6}+4*{1 \\over 6}+5*{1 \\over 6}+6*{1 \\over 6} = {21 \\over 6} $$ 다시 본론으로 돌아와, 보상함수는 왜 기댓값으로 표현하는 것일까? 보상을 에이전트에게 주는 것은 환경이고,환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수 있기 때문이다. 또한 보상함수에서 특이한 점은 에이전트가 어떤 상태에서 행동한 것은 시간 t에서인데 보상을 받는 것은 t+1이라는 것이다. 이는 보상을 에이전트가 알고 있는 것이 아니라 환경이 알려주기 때문이다. 때문에 에이전트가 받는 보상은 하나의 시간 단위가 지난 다음에 주어진다. 이 시간단위를 앞으로 타임스텝(time step)이라고 한다. 상태 변환 확률에이전트가 어떤 상태에서 어떤 행동을 취한다면 에이전트의 상태는 변할 것이다. $s’$은 다음 스텝에 에이전트가 갈 수 있는 어떤 특정한 상태를 의미하는데, 꼭 다음 상태에 도달하리라는 보장은 없다. 옆에서 바람이 불 수도 있고 갑자기 넘어질 수 있는 것이다. 이처럼 상태의 변화에는 확률적인 요인이 들어가고, 이를 수치적으로 표현한 것이 상태 변환 확률 $P$이다. $$ P*{ss’}^a = P[S*{t+1} = s’| S_t=s, A_t=a]$$ 상태 변환 확률은 상태 $s$에서 행동 $a$를 취했을 때 다른 상태 $s’$으로 도달할 확률이다. 이 값은 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부이다. 다른 말로 환경의 모델(model) 이라 부르기도 한다. 환경은 에이전트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이전트가 갈 상태를 알려준다. 할인율에이전트가 항상 현재에 판단을 내리기 때문에 현재에 가까운 보상일 수록 더 큰 가치를 지닌다. 가령 우리가 10억원 짜리 복권에 당첨되었다고 생각해보자. 지금 당장 받을 수도 10년 뒤에 받을 수도 있다고 할때, 우리는 당연히 당장 받는 것을 선호할 것이다. 시간이 지남에 따라 이자가 붙을 것을 가정하기 때문이다. 이는 다른 말로 같은 보상이면 나중에 받을수록 가치가 줄어든다 고 말할 수 있다. 강화학습에도 이와 같은 가정이 적용되고 이를 수학적으로 표현하기 위해 할인율(Discount Factor)이라는 개념을 도입한다. $$\\gamma \\in [0,1]$$ 할인율은 0과 1사이의 값이고, 보상에 곱해지면 보상이 감소한다. 이렇게 미래의 가치를 현재의 가치로 환산하는 것을 할인한다 고 하고, 시간에 따라 할인하는 비율을 할인율이라고 한다. 만약 현재의 시간 t로부터 시간 k가 지난 후에 보상을 $R_t+k$만큼 받을 것이라고 하면 그 보상의 가치는 아래와 같다 $$\\gamma^{k-1}R_{t+k}$$ 더 먼 미래에 받는 보상일수록 현재의 에이전트는 더 작은 값으로 받아들인다. 정책정책은 모든 상태에서 에이전트가 할 행동이다. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수 라고 생각해도 좋다. 정책은 각 상태에서 단 하나의 행동만을 나타낼 수도 있고, 확률적으로 $a_1 = 10%$, $a_2= 90%$와 같이 나타낼 수도 있다. 에이전트가 학습하고 있을 땐, 정책이 하나의 행동만을 선택하기 보다는 확률적으로 여러개의 행동을 선택할 수 있어야 하며 수식으로 나타내면 아래와 같다. $$ \\pi(a|s) = P[A_t = a|S_t=s]$$ 이러한 정책은 하나의 예시로서 각 상태마다 어떤 행동을 해야할지 아래의 그림과 같이 알려준다. 정책만 가지고 있으면 에이전트는 사실 모든 상태에서 자신이 해야 할 행동을 알 수 있다. 그러나 강화학습 문제를 통해 알고 싶은 것은 그냥 정책이 아니라 최적의 정책 이다. 최적 정책을 얻기 위해서 현재의 정책보다 더 좋은 정책을 학습해나가는 것이 강화학습이다. 정리이처럼 MDP를 통해 순차적 행동 결정 문제를 정의했다. 에이전트가 현재 상태에서 앞으로 받을 보상들을 고려해서 행동을 결정한다. 그러면 환경은 에이전트에게 실제 보상과 다음 상태를 알려준다. 이러한 과정을 반복하면서 에이전트는 어떤 상태에서 앞으로 받을 것이라 예상했던 보상에 대해 틀렸다는 것을 알게 된다. 이때 앞으로 받을 것이라 예상하는 보상을 가치함수(Value Function)라고 하며, 다음 장에서 설명된다. 그러한 과정에서 에이전트는 실제로 받은 보상을 토대로 가치함수와 정책을 바꿔나간다. 이러한 학습 과정을 충분히 반복한다면 가장 많은 보상을 받게하는 정책을 학습할 수 있다.","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"MDP","slug":"MDP","permalink":"https://longshiine.github.io/tags/MDP/"}]},{"title":"강화학습 개요","slug":"2021/01/19/강화학습-개요","date":"2021-01-19T07:09:49.000Z","updated":"2023-01-06T05:20:03.956Z","comments":true,"path":"2021/01/19/강화학습-개요/","link":"","permalink":"https://longshiine.github.io/2021/01/19/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B0%9C%EC%9A%94/","excerpt":"","text":"본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 첫번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413 방학이 어느새 1달여가 다 되어가는 시점이다.. 😥 1달간의 잉여생활을 청산하기 위해, 서점에 들러 강화학습 도서를 집었다. 원래는 CS234를 도전해보려 했으나, 미약한 영어실력 및 충전된 잉여력으로 인해 1강을 채 못 끝내었다는 불편한 진실..을 뒤로 하고, 코드와 함께 있는 이 도서를 정독해보기로 마음먹었다. 아무래도 나는 코드가 없으면 재미를 못느끼는 타입인가 보다. 암튼암튼. 이 책 만큼은 끝까지 도달하기를 진심진심으로 바란다. (교보문고에서 무려 2만 8천원을 고대로 내고 사왔다!) 1장. 강화학습 개요 이 책의 목표는 다음과 같다. 최소한의 수식과 직관적인 그림을 통해 강화학습을 이해하는 것 간단한 게임에 강화학습 이론을 직접 구현해보는 것 행동심리학과 머신러닝에 뿌리를 둔 강화학습에 대해 공부하려면 강화학습이 풀려고 하는 문제에 대해 정의를 먼저 해야한다. 강화학습은 다른 머신러닝 분야와 다르게 순차적으로 행동을 결정해야 하는 문제를 다루며 이러한 문제를 컴퓨터가 풀기 위해서는 문제를 수학적으로 잘 정의해야 한다. 스키너의 강화 연구강화(Reinforcement)는 동물이 시행착오(Trial and Error)를 통해 학습하는 방법 중 하나이고, 행동심리학의 시행착오 학습이라는 개념은 동물들이 이것저것 시도해보면서 그 결과를 통해 학습하는 것을 말한다. 굶긴 쥐를 상자에 넣는다. 쥐는 돌아다니다가 우연히 상자 안에 있는 지렛대를 누르게 된다. 지렛대를 누르자 먹이가 나온다. 지렛대를 누르는 행동과 먹이와의 상관관계를 모르는 쥐는 다시 돌아다닌다. 그러다가 우연히 쥐가 다시 지렛대를 누르면 쥐는 이제 먹이와 지렛대 사이의 관계를 알게 되고 점점 지렛대를 자주 누르게 된다. 이 과정을 반복하면서 쥐는 지렛대를 누르면 먹이를 먹을 수 있다는 것을 학습한다. 즉, 강화라는 것은 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상사이의 상관관계를 학습하는 것이다 머신러닝과 강화학습강화학습을 정의하려면 행동심리학의 강화라는 개념 이외에 머신러닝을 알아야 하는데 머신러닝은 다음과 같이 크게 세 가지로 나뉜다. 지도학습(Supervised Learning): 회귀분석(Regression), 분류(Classification) 비지도학습(Unsupervised Learning): 군집화(Clustering) 강화학습(Reinforcement Learning) 강화학습은 지도학습, 비지도학습과 그 성격이다르다. 정답이 주어진 것은 아니지만 그저 주어진 데이터에 대해 학습하는 것도 아니기 때문이다. 강화학습은 보상(reward) 을 통해 학습한다. 보상은 컴퓨터가 선택한 행동(Action) 에 대한 환경의 반응이다. 이 보상은 직접적인 답은 아니지만 컴퓨터에게는 간접적인 정답의 역할을 한다. 강화학습을 수행하는 컴퓨터는 행동심리학에서 살펴본 강화처럼 보상을 얻게 하는 행동을 점점 많이 하도록 학습한다. 스스로 학습하는 컴퓨터, 에이전트앞으로 강화학습을 통해 스스로 학습하는 컴퓨터를 에이전트(Agent)라고 할 것이다. 에이전트는 환경에 대해 사전지식이 없는 상태에서 학습을 한다. 에이전트는 자신이 놓인 환경에서 자신의 상태를 인식한 후 행동한다. 그러면 환경은 에이전트에게 보상을 주고 다음 상태를 알려준다(아래의 그림처럼 말이다!). 에이전트는 자신의 행동과 행동의 결과를 보상을 통해 학습하면서 어떤 행동을 해야 좋은 결과를 얻게 되는지 알게 된다. 강화학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 최적의 행동양식, 또는 정책을 학습하는 것이다. 강화학습의 장점이러한 강화학습의 장점은 무엇일까? “환경에 대한 사전지식이 없어도 학습한다는 것” 실제 세상에서 에이전트가 어떠한 기능을 학습하려면 다양한 상황에 대한 정보가 있어야 한다. 이러한 정보 없이 에이전트는 시행착오를 통해 어떠한 기능을 학습한다. 알파고에 대해 강화학습 관점에서 생각해보면, 알파고 또한 바둑이라는 게임의 규칙과 사전지식이 없는 상태에서 바둑을 두면서 학습한 것이다. 처음에는 무작위로 바둑돌을 놓다가 어쩌다가 상대방을 이기기 된다. 그러면 에이전트는 보상을 받고 상대방을 이기게 한 행동을 더 하려고 한다(실제로 알파고는 바둑을 학습할 때 사람이 둔 기보를 통해 지도학습을 하는 단계도 있지만 이 책에서는 해당 내용을 생략한다) 순차적 행동 결정 문제강화학습은 마치 사람처럼 환경과 상호작용하면서 스스로 학습하는 방식이다. 하지만 다른 머신러닝과 마찬가지로 강화학습은 문제 자체에 대해 잘 이해하지 않으면 엉뚱한 결과를 낳는다. 강화학습은 어떤 문제에 적용할까? “강화학습은 결정을 순차적으로 내려야 하는 문제에 적용된다” 결정을 순차적으로 내려야하는 문제라는 것은 예를 들어 현재 위치에서 행동을 한번 선택하는 것이 아니라 계속적으로 선택해야 하는 아래의 게임 같은 것이다. 에이전트가 문제에 대하여 학습하고 발전하려면 문제를 수학적으로 표현 해야한다. 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 MDP(Markov Decision Process)이다. MDP는 순차적 행동 결정문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 한다. 순차적 행동 결정 문제의 구성요소수학적으로 정의된 문제는 다음과 같은 구성요소를 가진다. 이 구성 요소들을 MDP라 부르며 2장에서 자세히 다룬다. 상태(state) 에이전트의 상태를 뜻하는데 이러한 상태에는 정적인 요소 뿐만아니라 에이전트가 움직이는 속도와 같은 동적인 요소 또한 포함된다. 가령 탁구를 치는 에이전트를 가정하면 탁구공의 위치, 속도, 가속도 같은 정보가 상태로 주어져야 한다. 행동(action) 에이전트가 어떠한 상태에서 취할 수 있는 행동으로서 “상”,”하”,”좌”,”우”와 같은 것을 말한다. 게임에서의 행동이라면 게임기를 통해 줄 수 있는 입력일 것이다. 학습이 되지 않은 에이전트는 어떤 행동이 좋은 행동인지에 대한 정보가 전혀 없다. 하지만 에이전트는 학습하면서 특정한 행동들을 할 확률을 높인다. 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려준다. 보상(reward) 보상은 강화학습을 다른 머신러닝 기법과 다르게 만들어주는 가장 핵심적인 요소이다. 사실상 에이전트가 학습할 수 있는 유일한 정보가 바로 보상이다. 앞서 언급했듯 강화학습의 목표는 시간에 따라 얻는 보상들의 합을 최대로 하는 정책을 찾는 것이다. 보상은 에이전트에 속하지 않는 환경의 일부이며, 에이전트는 어떤 상황에서 얼마의 보상이 나오는지에 대해 미리 알지 못한다. 정책(policy) 순차적 행동 결정 문제에서 구해야할 답은 바로 정책이다. 에이전트가 보상을 얻으려면 행동을 해야 하는데 특정 상태가 아닌 모든 상태에 대해 어떤 행동을 해야 할지 알아야 한다. 이렇게 모든 상태에 대해 에이전트가 어떤 행동을 해야하는지 정해놓은 것이 정책이다. 순차적 행동 결정 문제를 풀었다고 한다면 제일 좋은 정책을 에이전트가 얻었다는 것이다. 제일 좋은 정책은 최적정책(optimal policy)이라고 하며 에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대로 받을 수 있다. 강화학습의 예시: 브레이크 아웃이 책에서는 강화학습을 통해 몇가지 간단한 게임을 학습해본다. 그중에서 마지막 게임인 “브레이크 아웃” 즉, 벽돌깨기에 강화학습을 어떤 식으로 적용하는지 알아보자. 벽돌깨기에 강화학습을 적용하려면 어떻게 해야할까?이 말은 곧 벽돌깨기의 MDP를 어떻게 구성해야 할까? 라는 질문과도 같다. 또한 에이전트는 어떻게 학습을 해야할지에 대해서도 생각을 해보아야 한다. MDP 상태: 게임화면, 위의 그림과 같은 4개의 화면이 상태로 에이전트에게 제공되며, 이때 화면은 흑백화면이기 때문에 2차원 픽셀 데이터이다. 행동: 제자리, 왼쪽 오른쪽, 발사가 가능하고 발사는 시작 때에만 가능하다. 보상: 벽돌이 하나씩 깨질 때마다 보상을 (+1)씩 받고 더 위쪽을 깰수록 더 큰 보상을 받는다. 아무것도 깨지 않을 때는 보상으로 (0)을 바고, 공을 놓쳐 목숨을 잃는다면 (-1)을 받는다. 학습 에이전트는 4개의 연속된 게임 화면을 입력으로 받는다. 처음에는 아무것도 모르므로 임의로 행동을 취한다. 그에 따라 보상을 받게 되면 그 보상을 통해 학습한다. 결국 사람처럼 혹은 사람보다 잘하게 된다. 강화학습을 통해 학습되는 것은 인공신경망이다(인공신경망에 대해서는 5장에서 다뤄진다.) 인공신경망의 입력으로 위의 그림과 같은 4개의 연속적인 게임 화면이 들어온다. 인공신경망으로 입력이 들어오면 그 상태에서 에이전트가 할 수 있는 행동이 얼마나 좋은지 출력으로 내놓는다. 행동이 얼마나 좋은지가 행동의 가치가 되고 이것을 큐함수(Q Function)라고 한다. 이 문제에 사용한 인공신경망을 DQN(Deep Q-Network)라고 하는데 DQN에 상태가 입력으로 들어오면 DQN은 그 상태에서 제자리, 왼쪽, 오른쪽 행동의 큐함수를 출력으로 내놓는다. 에이전트는 출력으로 나오는 큐함수에 따라서 행동한다. 즉, DQN이 출력한 큐함수를 보고 큰가치를 지니는 행동을 선택하는 것이다. 에이전트가 그행동을 취하면 환경은 에이전트에게 보상과 다음 상태를 알려준다. 에이전트는 환경과 상호작용하면서 DQN을 더 많은 보상을 받도록 조금씩 조정한다. 에이전트는 이와 같은 방식으로 벽돌깨기를 학습하는데, 이 예제에 대한 자세한 이론과 코드는 뒤에서 다루어진다. 여기서는 어떤 흐름으로 에이전트가 강화학습을 통해 학습하는 지를 아는 것이 목적이다. 사람과 강화학습 에이전트의 차이에이전트가 강화학습을 통해 벽돌깨기를 학습하는 것은 사람의 학습 과정과 비슷한 면이 있다. 비슷한 점은 사람이 게임 화면을 보고 학습해 나가듯이 에이전트 또한 화면을 보고 학습한다는 점이다. 하지만 사람과 다른 점 또한 있다. 그것은 에이전트는 게임의 규칙을 전혀 모른다는 것이다. 아마도 처음 벽돌깨기를 하는 사람이 있다면 게임을 시작하기 이전에 먼저 규칙이 무엇인지를 찾아본 뒤, 의도를 가지고 점수를 올려나갈 것이다. 어떻게 보면 게임의 규칙을 몰라도 학습할 수 있다는 것은 강화학습의 장점이면서도 초반의 느린 학습의 원인이기도 하다. 잘하는 친구가 옆에서 가르쳐준다면 더 빠르게 배울 수 있지 않을까? 사람은 하나를 학습하면 다른곳에도 그 학습이 영향을 미친다. 예를 들어, 어떤학생이 수학을 배웠다면 과학을 배우기에도 더 수월한 것 처럼. 하지만 현재 강화학습 에이전트는 각 학습을 다 별개로 취급해서 항상 바닥부터 학습해야 한다. 이렇게 간단히 살펴본 사람과 강화학습 에이전트의 차이는 현재 및 미래 강화학습 분야의 연구 분야로서 지속적으로 해결해야 할 과제이다. 1장 한줄평 블로그 포스팅 너무 빡세다.. 연습 필요..","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"}],"tags":[{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"}]},{"title":"Optimal Auction Through Deep Learning(1)","slug":"2021/01/03/Optimal-Auction-Through-Deep-Learning-1","date":"2021-01-03T08:34:28.000Z","updated":"2023-01-05T10:06:44.748Z","comments":true,"path":"2021/01/03/Optimal-Auction-Through-Deep-Learning-1/","link":"","permalink":"https://longshiine.github.io/2021/01/03/Optimal-Auction-Through-Deep-Learning-1/","excerpt":"","text":"본 포스트는 “Auction Theory” 전반과 “Optimal Auction Through Deep Learning” 논문에 대한 리뷰 포스트입니다.https://arxiv.org/abs/1706.03459 사실 경제학을 공부해본 적이 없기에 경매이론 자체를 다루는 데에는 한계가 있었다. 첫번째 포스트에서는 경매에 대해 간단하게 살펴보고, 경매이론이 어떤 이유로 노벨상을 수상했는지, 어떤식으로 활용이 될 수 있는지를 다뤄보고자 한다. 경매, 난해한 수학 퍼즐경매는 오늘날 어디서나 흔히 볼 수 있는 거래 형태다. 여기서 매입자는 특정 상품을 사들이기 위해 계속 가격을 높여가며 제시한다. 이베이(eBay)에 나온 기타, 마이해머(MyHammer) 같은 플랫폼에서 제공하는 여러 서비스, 아주 전통적인 예로 옥션 매장에서 거래되는 예술품 등은 바로 이런 방식으로 매매된다. 이렇게 일반에 잘 알려진 상행위이지만, 연구 대상으로서 경매의 세부 규칙을 다루는 순간, 경매는 대번에 고도로 복잡한 주제가 된다. 경매 규칙 연구가 얼마큼 복잡한 사안이 될 수 있는지, 미국 스탠퍼드대학 경제학과 로버트 윌슨 교수와 폴 밀그럼 교수의 예가 잘 보여준다. 경매 규칙을 총체적으로 파악한 뒤 이를 바탕으로 탁월한 경매 방식을 고안한 공적으로, 두 연구자는 2020년 노벨경제학상을 공동 수상했다. 결정할 때 작용하는 여러 문제를 수학적으로 분석하는 놀이이론 분야에서, 경매는 가장 단순한 형태로 실행될지라도 분석이 특별히 어려운 주제로 꼽힌다. 경매 분석은 1996년, 1994년 각각 노벨경제학상을 받은 윌리엄 비크리와 존 허샤니의 논문을 통해, 1960년대에 경제이론의 한 부분으로 정립됐다. 두 학자의 연구를 발전시키면서, 로버트 윌슨과 그의 제자인 폴 밀그럼은 학계에 깊은 인상을 남긴 영향력 있는 이론을 연이어 발표했다. 이 논문들은 여러 경제 분야에서 경매가 각각 어떤 식으로 형성되는지 파악하는 중요한 토대를 마련해놓고 있다. 경매 과정에서 보이는 입찰자 행동을 이해하게 해주는 이 연구는 다양한 경매 규칙을 새롭게 평가하는 토대도 마련해준다. 밀그럼과 윌슨의 연구는 오늘날 더욱 중요한 의미를 갖는다. 날로 디지털화하는 요즘, 경제활동 전역에서 경매 방식이 갈수록 더 복잡해지고 있기 때문이다. 무선 이동통신망 경매나 어업수역 경매, 온라인경매, 에너지시장 경매, 산업 분야 공급업체 경매는 전체 경매 방식의 일부에 불과하다. 주파수 경매일반의 경매에 관한 관심이 고조된 때는 1990년대였다. 당시 세계 어느 정부를 막론하고 자국 재량권 내에 있는 무선 광대역통신망을 경매로 분배할 방법을 모색하고 있었다.이동통신사에는 수십억원대 장사를 할 가능성이 코앞에 다가온 셈이었다. 국가로서도 막대한 세금을 벌어들일 기회였다. 동시에 경매를 통하면 국가는 광대역통신망을 관리할 최상의 기업을 선택하는 이점이 있었다. 사업모델이 우수하면 허가를 얻기 위해 입찰자가 제시하는 기준도 그만큼 더 높을 것이다. 이뿐만이 아니다. 마지막에 한 기업이 통신망 모두를 독점하는 상황을 초래하지 않으려면 국가보조금을 여러 기업에 나눠 지급해야 하는데, 이 또한 경매로 이루겠다는 게 정부 계획이었다. 물론 광대역통신망의 모든 주파수를 임의로 다 조합할 수 없다는 기술적 제한이 있지만 말이다. 통신사 선정은 매우 복합적인 문제였다. 어떤 규칙을 써야 최선의 입찰자가 낙찰받게 할 수 있을지, 선정 기업에 기술상 실행 가능한 주파수를 조합해서 주고 동시에 국고에도 최고 수익이 돌아오게 할 방법이 무엇인지를 알아내야 했다. 경매이론가 가운데 밀그럼과 윌슨은 각국 정부의 상담역을 맡아 정교한 규칙을 개발한 학자로 꼽힌다. 다른 학자들은 주로 기업을 상담하면서 입찰에서 최고의 결과를 얻는 전략을 개발하는 역할을 맡았다. 밀그럼과 윌슨은 오래전부터 그들의 경매 규칙을 뒷받침할 이론적 토대를 탄탄히 마련해놓았다.경매에서 개별 입찰자가 일부 동일한 기초 자료에 의존해서 입찰가를 결정한다는 사실을 밝혀내 경매 이해도를 높인 것은 두 연구자의 중요한 업적으로 평가된다. 결과적으로, 입찰 대상에 대해 한 입찰자가 좋은 평가를 하면 다른 입찰자의 평가도 좋게 나올 확률이 높다는 뜻이다. 따라서 매도자의 과제는 입찰자 가운데 최상의 사업모델, 혹은 저렴한 비용으로 이윤을 가장 많이 남길 수 있는 지원자가 누구일지 심사하는 것이다. 논리적으로 생각하면, 이 입찰자가 가장 높은 입찰가를 제시할 것이다. 여기에서 다음 같은 이의가 제기될 수도 있다. 최고 입찰가를 부른 매입자가 가장 뛰어난 사업모델을 가졌기 때문이 아니라, 단지 사업권의 가치를 과대평가했기 때문일 수도 있지 않은가. 이 경우 사업권을 따낸 회사는 비록 입찰에는 성공했지만 결국 지나치게 많은 금액을 냈음을 깨닫는다. 경매이론에선 이 경우를 ‘승자의 저주’라고 일컫는다.그러나 이성적인 입찰자는 이런 함정에 빠지지 않는다는 게 밀그럼과 윌슨의 시각이다. 경매장에서의 성공을 나중에 후회하는 일을 피하기 위해 입찰자들은 처음부터 나름 정해놓은 금액을 염두에 두고 경매에 나선다는 사실을, 두 학자는 분명히 보여줬다. 공개 경매 방식이 유리한 이유가 바로 이 때문이다. 경쟁자가 한 명씩 입찰을 포기할 때마다, 입찰자는 자기가 매입하려는 사업권을 다른 사람들이 어떻게 평가하는지 조금씩 더 알아간다. 이 추가 정보를 바탕으로 경우에 따라 대담하게 입찰 가격을 제시할 수도 있다. 동시 다중 라운드 경매 방식 개발단일 사업권을 공개 경매하는 것으로는 이 효과를 거둘 수 없다. 고객에게 매력적인 상품을 제공하기 위해 이동통신사에 전체 사업권 목록이 필요할 때가 자주 있다. 이 경우, 예를 들어 여러 지역을 함께 아우르는 사업권을 제공하는 것이 유리하다. 이외엔 어떤 사업권이 더 경매에 부쳐지는지 분명하지 않은 상황에선 개별 사업권의 가치를 측정하기 어렵다. 이 상황에선 다수 사업권의 동시 경매가 필요하다. 이 연구를 바탕으로, 밀그럼과 윌슨은 새로운 경매 방식을 개발했다. 공개 입찰 과정에서 여러 지방의 주파수 광대역 입찰을 동시에 여러 라운드를 걸쳐 진행하는 것이다. 이른바 ‘동시 다중 라운드 경매’(Simultaneous Multiple Round Auction)다. 이 방식을 사용하면 입찰자들이 개별적으로 보유한 정보가 정교하게 계산된 규칙에 따라 경매가 진행되는 과정에서 점차 하나로 모인다. 그 결과, 입찰자들이 원치 않는 형태로 조합된 사업권을 받아들거나 승자의 저주에 걸릴 위험이 줄어든다. 그렇게 되면 예상 경매 수익이 높아져서 경매자에게도 혜택이 돌아간다. 폴 밀그럼과 로버트 윌슨 교수의 이 아이디어는 전세계 주파수 광대역 경매 형태에 큰 영향을 주었다. 무엇보다 중요한 공적은 삶의 모든 영역에서 일어나는 무수한 경매에 적용되는 기초를 놓았다는 것이다. 2020년 노벨상 이야기는 기초연구 분야의 수학 모델이 지닌 우아함과 그 모델을 실생활에 적용하는 일이 경제학에서 서로 얼마나 가까이 접근할 수 있는지 인상적으로 보여준다.","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"딥러닝","slug":"Tech/딥러닝","permalink":"https://longshiine.github.io/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"},{"name":"OptimalAuction","slug":"OptimalAuction","permalink":"https://longshiine.github.io/tags/OptimalAuction/"}]},{"title":"Dot Product 정리","slug":"2020/09/04/Dot-Product","date":"2020-09-04T05:07:47.000Z","updated":"2023-01-06T06:35:50.479Z","comments":true,"path":"2020/09/04/Dot-Product/","link":"","permalink":"https://longshiine.github.io/2020/09/04/Dot-Product/","excerpt":"","text":"공학수학 수업 중 등장한 내적(Dot Product)의 개념에 대해 정리를 해보려 한다. 내적(Dot Product)의 정체먼저 살펴볼것은 벡터를 내적한다는 것이 무슨 의미인지에 관한 것이다. 내적 | 內積 | inner product | scalar product 적은 쌓는다는 뜻의 한자이고, 여기서는 곱한다는 뜻이다. 벡터의 곱하기는 두 가지 정의가 있는데, 내적은 벡터를 마치 수처럼 곱하는 개념이다. 그렇다 우리는 현재 벡터간의 곱하기를 하고 싶은 것이다. 물리적인 일(Work)의 개념에서 내적 이해하기우리가 일을 한다라는 것은 노동을 한다고 할 수도 있지만, 물리학적으로 일을 한다는 것은 물체에 힘을 주어서 적당한 거리 만큼 이동하였을 때, 그게 얼마나 되는지를 측정하는 것이라 볼 수 있다. 예를 들어 내가 나무토막에 F만큼의 힘을 주어서, s의 변위만큼 이동시켰다라고 이야기한다면, 내가 물체에 대해 한 일의 개념인 W는 아래와 같다 나 그런데 문제는 힘의 방향과 이동방향이 일치하지 않을 때의 경우인데, 아래의 그림처럼 이동한다고 생각해보자. 우리가 물리학적으로 일을 한다는 개념의 전제 중 하나는 ‘힘의 방향’과 ‘이동 방향’이 서로 동일할 경우에만 일을 한다고 말할 수 있다는 것이다. 전제에\u001d 따라 분석해보면 우리는 이 상황에서의 힘 F를 다음과 같이 분해해볼 수 있다. 이때 이동방향과 같은 힘만이 일에 작용하므로 수직으로 가해진 힘은 물체에 일을 해준 것이 아니게 된다.따라서 실제 아저씨가 물체에 대해 한 일은 아래와 같다. 아저씨 핵심은 여기서 말한 F와 s가 모두, 방향성이 있는 벡터 값이라는 것이다. F는 힘의 크기와 방향이 존재하고, s 또한 얼마만큼 이동할 것인가(크기) 어디로 이동할 것인가(방향) 등의 요소가 존재한다. 이 두 벡터량에서, 우리는 위의 일 개념에 타당할 수 있는 내적을 정의할 수 있는데, 바로 F,s의 내적을 다음과 같이 정의하고, 이동 방향, 작용하는 힘 사이의 각을 라고 할 때, 로 정의할 수 있는 것이다. 만약 첫번째 그림처럼 힘의 방향과 이동 방향이 같은 경우엔, 그 방향으로 100% 쓰인 것이므로 아래의 식처럼 되고, 나머지 경우는 각도에 해당하는 만큼이 곱해져서, 일의 정의에 맞는 물리량을 계산해 낼 수 있다. 내적의 기하학적 정의내적을 이제 물리 개념이 아닌 일반적인 벡터에 적용해보면, 두 벡터 a,b에 대해 a,b가 이루는 각을 라 할 때, 두 벡터의 내적을 다음과 같이 정의한다. 벡터 내적의 결과물은 스칼라 값이다. 벡터는 방향과 크기가 둘다 있고, 스칼라는 크기만 가지고 있음에 주의해야 한다. 벡터의 내적이라 함은 두 벡터끼리 곱하는 건데, 그 결과는 방향이 없는 스칼라 값이다…라고 해석해야 한다는 것이다. 우항의 인자들은 모두 스칼라 값이고, 이를 이용하면 내적의 교환법칙을 증명해 낼 수도 있다. 내적의 대수학적 정의앞서 물리학적으로 내적을 알아봄과 동시에 기하학적 정의를 도출해 보았다.그런데, 우리는 벡터를 성분으로 표현하는 법 역시 안다. 당연히 내적들도 연산을 성분으로 표현하는 방법을 생각할 수 있다. 내적의 속성 제2코사인 법칙을 이용한 증명","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"공학수학","slug":"Tech/공학수학","permalink":"https://longshiine.github.io/categories/Tech/%EA%B3%B5%ED%95%99%EC%88%98%ED%95%99/"}],"tags":[{"name":"Vector","slug":"Vector","permalink":"https://longshiine.github.io/tags/Vector/"},{"name":"Dot Product","slug":"Dot-Product","permalink":"https://longshiine.github.io/tags/Dot-Product/"},{"name":"Inner Product","slug":"Inner-Product","permalink":"https://longshiine.github.io/tags/Inner-Product/"}]},{"title":"Convolution Layer 구현","slug":"2020/08/29/Convolution-Layer","date":"2020-08-29T06:40:48.000Z","updated":"2023-01-06T06:31:42.969Z","comments":true,"path":"2020/08/29/Convolution-Layer/","link":"","permalink":"https://longshiine.github.io/2020/08/29/Convolution-Layer/","excerpt":"","text":"cs231n assignment2의 Convolution Layer를 구현해보며 느낀점을 정리해보았다.개념보다는 구현에 초점을 맞춘 포스트이므로 참고바람. Convolution LayerCNN의 핵심은 convolution operation 즉, 합성곱 연산이다. 기존의 Fully Connected Layer와 다른 Convolution Layer의 특징은 Input의 Spatial Structure, 공간적 구조를 보존한다는 것이다. Convolution 연산을 수행하면 input에 대한 feature map을 뽑을 수 있는데, filter의 개수에 따라 output인 activation map의 depth가 달라지며, filter가 어떤식으로 input을 sliding 하는지에 따라 activation map의 size가 변화한다. input_size(N), filter_size(F), padding(p) 등의 조건이 있다면 다음의 식으로 간단하게 Output size를 알아낼 수 있다. 그럼 정리해보자면,Convolution layer란 녀석은 input을 각 필터로 sliding하면서 값을 계산하기만 하면 되는 짜기 easy한 녀석이 아닐까? 맞긴한데 짜면서 아주 호되게 혼났다. Forward Pass123456789101112131415161718192021def conv_forward_naive(x, w, b, conv_param):\"\"\"Input:- x: Input data of shape (N, C, H, W)- w: Filter weights of shape (F, C, HH, WW)- b: Biases, of shape (F,)- conv_param: A dictionary with the following keys: - 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions. - 'pad': The number of pixels that will be used to zero-pad the input.Returns a tuple of:- out: Output data, of shape (N, F, H', W') where H' and W' are given by H' = 1 + (H + 2 * pad - HH) / stride W' = 1 + (W + 2 * pad - WW) / stride- cache: (x, w, b, conv_param)\"\"\"out = Nonecache = (x, w, b, conv_param)return out, cache 문제를 조금 해석해보자면, size가 H x W x C(channel의 수, rgb 같은) 인 이미지 N개가 input x로 들어왔다. input을 size가 HH x WW x C인 필터 F개로 sliding 하고, size가 F인 bias도 더해서, size가 H’x W’x F인 out N개로 만들어내라. 이때 H’, W’은 위에서 살펴보았던 식으로 잘 계산해라 요런 문제이다.그럼 computation graph를 한번 생각해보자. 다이렉트로 out이 나오는 그래프는 아니다.먼저 input x에 padding을 추가해주고, filter의 크기만큼을 crop한 뒤, filter와 내적을 취해주고, bias term을 더해주는 과정이다.이 과정을 거치고 나면 (N,F)의 spatial_out이 계산되는 데, 요 그림에서 가운데 공이 담긴 녀석(공이 각각의 필터가 계산한 값이고, F개 있다고 보면 됨)이 N개 있는 꼴이라고 생각하면 된다. 이 과정을 H’xW’ 번 해주면 우리가 원하는 out(N,F,H’,W’)을 얻어낼 수 있는 것이다! 12345678910111213141516171819202122N, C, H, W = x.shapeF, C, HH, WW = w.shapestride = conv_param['stride']pad = conv_param['pad']npad = ((0,0), (0,0), (pad,pad), (pad,pad)) # padding 위한 값filter_size = C*HH*WW # 내적의 용이성을 위해 미리 계산해두는 값H_out = int(1 + (H + 2 * pad - HH) / stride) # H'W_out = int(1 + (W + 2 * pad - WW) / stride) # W'out = np.zeros((N, F, H_out, W_out)) # 최종적으로 구할 out 초기화 (N, F, H', W')x_pad = np.pad(x, npad, 'constant', constant_values=(0)) # x에 pad 크기만큼 zero-paddingfor height in range(H_out): for width in range(W_out): x_crop = x_pad[np.arange(N), :, height*stride:height*stride+HH, width*stride:width*stride+WW] # x에서 (N,C,HH,WW)크기만큼을 crop x_crop_stretch = x_crop.reshape(N, filter_size) # (N, filter_size) w_stretch = w.reshape(F, filter_size) # (F, filter_size) spatial_out = np.dot(x_crop_stretch, w_stretch.T) + b.reshape((1,F)) # (N,F) out[np.arange(N), :, height, width] = spatial_out Backward Pass포워드 열심히 짰으니 이젠 backward 짤 차례다.쫄면 안된다. (이자는 쫄았다) 12345678910111213def conv_backward_naive(dout, cache): \"\"\" Inputs: - dout: Upstream derivatives. - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive Returns a tuple of: - dx: Gradient with respect to x - dw: Gradient with respect to w - db: Gradient with respect to b \"\"\" dx, dw, db = None, None, None return dx, dw, db 문제는 참 심플하다.아까 그려놓은 computational graph보면서 dx, dw, db를 구하면 된다. 12345678910111213141516171819202122232425262728293031x, w, b, conv_param = cacheN, C, H, W = x.shapeF, C, HH, WW = w.shapeN, F, H_out, W_out = dout.shapestride = conv_param['stride']pad = conv_param['pad']npad = ((0,0), (0,0), (pad,pad), (pad,pad)) # x = (N, C, H+2*pad, W+2*pad)filter_size = C*HH*WW # scalar, for stretchx_pad = np.pad(x, npad, 'constant', constant_values=(0))dx_pad = np.zeros((N,C,H+2*pad,W+2*pad))dw = np.zeros((F,C,HH,WW))db = np.zeros(F)for height in range(H_out): for width in range(W_out): dspatial_out = dout[np.arange(N), :, height, width] # (N, F) db += np.sum(dspatial_out, axis=0) # (F, ) x_tmp = x_pad[np.arange(N), :, height*stride:height*stride+HH, width*stride:width*stride+WW] # (N, C, HH, WW) dw_stretch = np.dot(dspatial_out.T, x_tmp.reshape(N, filter_size)) # (N,F).T @ (N,filter_size) = (F, filter_size) dw += dw_stretch.reshape(F,C,HH,WW) w_stretch = w.reshape(F, filter_size) # (F, filter_size) dx_tmp_stretch = np.dot(dspatial_out, w_stretch) # (N,F) @ (F,filter_size) = (N, filter_size) dx_tmp = dx_tmp_stretch.reshape(N,C,HH,WW) dx_pad[np.arange(N), :, height*stride:height*stride+HH, width*stride:width*stride+WW] += dx_tmpdx = dx_pad[np.arange(N), :, pad:H+pad, pad:W+pad] # padding 제거한 값 간략히 설명하자면, spatial_out을 구해서 전체 out을 만들어 냈던 것처럼, dout에서 dspatial_out(N,F)을 crop하여 gradient를 계산해 나간다. 주의해야 할 것은 padding된 x값을 이용해 만들어 낸 out 이었기 때문에, dx_pad에 gradient를 계산해 놓고, padding을 제거한 값을 dx에 할당해야 한다는 것이다. backward 그래프도 그렸으나, 너무 지저분해서 올리진 않겠다..(아이패드 갖고 싶습니다…) 느낀점 4중 for문 돌릴 뻔했다. 무엇을 기준으로 sliding 해나갈지가 너무 애매해서 일단 N, F, H, W 모두 돌렸다.근데 그렇게 한번 짜고나니, N, F에 대해서는 Vectorize의 가능성이 보였고, 짰고, 돌아갔다. 짜릿하더라. 뭐 요정도 일 것 같다.","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"딥러닝","slug":"Tech/딥러닝","permalink":"https://longshiine.github.io/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://longshiine.github.io/tags/CNN/"},{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"}]},{"title":"Gradient Vanishing이란?","slug":"2020/08/27/Gradient-Vanishing","date":"2020-08-27T07:07:41.000Z","updated":"2023-01-05T10:06:37.811Z","comments":true,"path":"2020/08/27/Gradient-Vanishing/","link":"","permalink":"https://longshiine.github.io/2020/08/27/Gradient-Vanishing/","excerpt":"","text":"Gradient Vanishing Problem(기울기값이 소실)는 인공신경망을 기울기값을 베이스로 하는 method (back propagation)로 학습시키려고 할 때 발생되는 어려움이다. 특히 이 문제는 네트워크에서 앞쪽 레이어의 파라미터들을 학습시키고, 튜닝하기 정말 어렵게 만든다 (Backprop은 뒤에서 앞으로 진행되므로 중간에 기울기가 소실되면 앞단 layer들의 업데이트가 안된다). 이 문제는 신경망 구조에서 레이어가 늘어날수록 더 악화된다. 이것은 뉴럴 네트워크의 근본적인 문제점이 아니다. 이것은 특정한 activation function를 통해서 기울기 베이스의 학습 method를 사용할 때 문제가 된다. 자 한번 이러한 문제를 직관적으로 이해해보고, 그것으로 인해 야기되는 문제를 짚어보자. ProblemGradient 기반의 method는 parameter value의 작은 변화가 network output에 얼마나 영향을 미칠지를 이해하는 것을 기반으로 parameter value를 학습시킨다. 만약 parameter value의 변화가 network’s output의 매우 작은 변화를 야기한다면, 네트워크는 parameter를 효과적으로 학습시킬 수 없게 되는데 이것이 문제다. gradient라는 것이 결국 미분값 즉 변화량을 의미하는데, 이 변화량이 매우 작다면, network 를 효과적으로 학습시키지 못하고, error rate이 미쳐 다 낮아지지 못한채 수렴해버리는 문제가 발생한다는 것 같다. 이 상황이 바로 vanishing gradient problem으로 발생하는 문제인데, 이 문제로 초기 레이어에서 각각의 parameter들에 대한 network’s output의 gradient는 극도로 작아지게 된다. 이걸 좀 근사하게 얘기하면, 초기 레이어에서 parameter value에 대해 큰 변화가 발생해도 output에 대해서 큰 영향을 주지 못한다는 것이다. 그렇다면, 언제, 왜 이러한 문제가 발생하는지 살펴보자. CauseVanishing gradient problem은 activation function을 선택하는 문제에 의존적으로 일어난다. sigmoid나, tanh 등 요즘 많이들 사용하는 activation function들은 매우 비선형적인 방식으로 그들의 input을 매우 작은 output range로 짓이겨넣는다 예를 들어서, sigmoid는 실수 범위의 수를 [0, 1]로 맵핑한다. 그 결과로 매우 넓은 input space 지역이 극도로 작은 범위로 맵핑되어버린다. 이렇게 되어 버린 input space에서는 큰 변화가 있다고 하더라도, output에는 작은 변화를 보이게 된다. gradient(기울기)가 작기 때문이다. 이러한 현상은 우리가 서로(??)의 꼭대기 층에 그러한 비선형성을 여러개 레이어로 쌓을 때 더욱 악화된다. 예를들어, 첫 레이어에서 넓은 input region을 작은 output region으로 맵핑하고, 그것이 2차 3차 레이어로 갈수록 더 심각하게 작은 region으로 맵핑되는 경우이다. 그 결과로 만약 첫 레이어 input에 대해 매우 큰 변화가 있다고 하더라도 output을 크게 변화시키지 못하게 된다. 우리는 이러한 문제를 해결하기 위해 짓이겨 넣는식(‘squashing’)의 특징을 갖지 않는 activation function을 사용할 수 있다. ReLU(Rectified Linear Unit - max(0, x))가 잘 선택되는 편이다. 참고 https://www.quora.com/What-is-the-vanishing-gradient-problem","categories":[{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"딥러닝","slug":"Tech/딥러닝","permalink":"https://longshiine.github.io/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"},{"name":"GradientVanishing","slug":"GradientVanishing","permalink":"https://longshiine.github.io/tags/GradientVanishing/"}]}],"categories":[{"name":"Writing","slug":"Writing","permalink":"https://longshiine.github.io/categories/Writing/"},{"name":"Tech","slug":"Tech","permalink":"https://longshiine.github.io/categories/Tech/"},{"name":"딥러닝","slug":"Tech/딥러닝","permalink":"https://longshiine.github.io/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"알고리즘","slug":"Tech/알고리즘","permalink":"https://longshiine.github.io/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/"},{"name":"강화학습","slug":"Tech/강화학습","permalink":"https://longshiine.github.io/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/"},{"name":"공학수학","slug":"Tech/공학수학","permalink":"https://longshiine.github.io/categories/Tech/%EA%B3%B5%ED%95%99%EC%88%98%ED%95%99/"}],"tags":[{"name":"스타트업","slug":"스타트업","permalink":"https://longshiine.github.io/tags/%EC%8A%A4%ED%83%80%ED%8A%B8%EC%97%85/"},{"name":"회고","slug":"회고","permalink":"https://longshiine.github.io/tags/%ED%9A%8C%EA%B3%A0/"},{"name":"Travel","slug":"Travel","permalink":"https://longshiine.github.io/tags/Travel/"},{"name":"Nepal","slug":"Nepal","permalink":"https://longshiine.github.io/tags/Nepal/"},{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://longshiine.github.io/tags/DeepLearning/"},{"name":"ComputerVision","slug":"ComputerVision","permalink":"https://longshiine.github.io/tags/ComputerVision/"},{"name":"GAN","slug":"GAN","permalink":"https://longshiine.github.io/tags/GAN/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://longshiine.github.io/tags/Algorithm/"},{"name":"PriorityQueue","slug":"PriorityQueue","permalink":"https://longshiine.github.io/tags/PriorityQueue/"},{"name":"heapq","slug":"heapq","permalink":"https://longshiine.github.io/tags/heapq/"},{"name":"DFS","slug":"DFS","permalink":"https://longshiine.github.io/tags/DFS/"},{"name":"BFS","slug":"BFS","permalink":"https://longshiine.github.io/tags/BFS/"},{"name":"일기","slug":"일기","permalink":"https://longshiine.github.io/tags/%EC%9D%BC%EA%B8%B0/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://longshiine.github.io/tags/ReinforcementLearning/"},{"name":"SARSA","slug":"SARSA","permalink":"https://longshiine.github.io/tags/SARSA/"},{"name":"QLearning","slug":"QLearning","permalink":"https://longshiine.github.io/tags/QLearning/"},{"name":"MonteCarloApproximation","slug":"MonteCarloApproximation","permalink":"https://longshiine.github.io/tags/MonteCarloApproximation/"},{"name":"TemporalDifference","slug":"TemporalDifference","permalink":"https://longshiine.github.io/tags/TemporalDifference/"},{"name":"PolicyIteration","slug":"PolicyIteration","permalink":"https://longshiine.github.io/tags/PolicyIteration/"},{"name":"ValueIteration","slug":"ValueIteration","permalink":"https://longshiine.github.io/tags/ValueIteration/"},{"name":"DynamicProgramming","slug":"DynamicProgramming","permalink":"https://longshiine.github.io/tags/DynamicProgramming/"},{"name":"GridWorld","slug":"GridWorld","permalink":"https://longshiine.github.io/tags/GridWorld/"},{"name":"BellmanEquation","slug":"BellmanEquation","permalink":"https://longshiine.github.io/tags/BellmanEquation/"},{"name":"ValueFunction","slug":"ValueFunction","permalink":"https://longshiine.github.io/tags/ValueFunction/"},{"name":"MDP","slug":"MDP","permalink":"https://longshiine.github.io/tags/MDP/"},{"name":"OptimalAuction","slug":"OptimalAuction","permalink":"https://longshiine.github.io/tags/OptimalAuction/"},{"name":"Vector","slug":"Vector","permalink":"https://longshiine.github.io/tags/Vector/"},{"name":"Dot Product","slug":"Dot-Product","permalink":"https://longshiine.github.io/tags/Dot-Product/"},{"name":"Inner Product","slug":"Inner-Product","permalink":"https://longshiine.github.io/tags/Inner-Product/"},{"name":"CNN","slug":"CNN","permalink":"https://longshiine.github.io/tags/CNN/"},{"name":"GradientVanishing","slug":"GradientVanishing","permalink":"https://longshiine.github.io/tags/GradientVanishing/"}]}