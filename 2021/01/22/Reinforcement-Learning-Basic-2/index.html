<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    

    
    <title>강화학습 기초: 가치함수와 벨만방정식 | Jang. Inspiration</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="ReinforcementLearning,BellmanEquation,ValueFunction" />
    
    <meta name="description" content="본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다.http:&#x2F;&#x2F;www.yes24.com&#x2F;Product&#x2F;Goods&#x2F;44136413  2장 강화학습 기초 1: 가치함수와 벨만 방정식 이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의 했다. 에이전트는 MDP를 통해 최적 정책 을 찾으면 된다. 이제 에이전트가 어떻게 최적">
<meta property="og:type" content="article">
<meta property="og:title" content="강화학습 기초: 가치함수와 벨만방정식">
<meta property="og:url" content="https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/index.html">
<meta property="og:site_name" content="Jang. Inspiration">
<meta property="og:description" content="본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다.http:&#x2F;&#x2F;www.yes24.com&#x2F;Product&#x2F;Goods&#x2F;44136413  2장 강화학습 기초 1: 가치함수와 벨만 방정식 이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의 했다. 에이전트는 MDP를 통해 최적 정책 을 찾으면 된다. 이제 에이전트가 어떻게 최적">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/qfunction.jpeg">
<meta property="article:published_time" content="2021-01-22T07:12:02.000Z">
<meta property="article:modified_time" content="2023-01-06T05:25:07.791Z">
<meta property="article:author" content="Jangyeong Kim">
<meta property="article:tag" content="ReinforcementLearning">
<meta property="article:tag" content="BellmanEquation">
<meta property="article:tag" content="ValueFunction">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/qfunction.jpeg">
    

    

    
        <link rel="icon" href="/favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.5.0/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R6RW6NYPPH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-R6RW6NYPPH');
</script>
<!-- End Google Analytics -->

    
    
    


<meta name="generator" content="Hexo 5.4.2"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                    <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Tech/">Tech</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/">강화학습</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Tech/%EA%B3%B5%ED%95%99%EC%88%98%ED%95%99/">공학수학</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/">알고리즘</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Writing/">Writing</a></li></ul>
                                
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/Tech/">Tech</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/">강화학습</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-2021/01/22/Reinforcement-Learning-Basic-2" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        강화학습 기초: 가치함수와 벨만방정식
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2021/01/22/Reinforcement-Learning-Basic-2/" class="article-date">
       <time datetime="2021-01-22T07:12:02.000Z" itemprop="datePublished">Jan 22, 2021</time>
    </a>
  </div>

<!-- 
<div class="article-date">
  <i class="fa fa-calendar-plus-o"></i>
  <a href="/2021/01/22/Reinforcement-Learning-Basic-2/" class="article-date">
     <time datetime="2023-01-06T05:25:07.791Z" itemprop="dateModified">Jan 06, 2023</time>
  </a>
</div>
 -->

                
   <div class="article-counter">
      <!-- <i class="fa fa-file-word-o"></i>
      <span class="post-count-item-text">Symbols count in article: </span>
      <span class="post-count">1.9k</span> -->
      <i class="fa fa-clock-o"></i>
      <!-- <span class="post-time-item-text">Reading time: </span> -->
      <span class="post-count">12 mins. </span>
   </div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/BellmanEquation/" rel="tag">BellmanEquation</a>, <a class="tag-link-link" href="/tags/ReinforcementLearning/" rel="tag">ReinforcementLearning</a>, <a class="tag-link-link" href="/tags/ValueFunction/" rel="tag">ValueFunction</a>
    </div>

                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            

            

            

            <blockquote>
<p>본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다.<br><a target="_blank" rel="noopener" href="http://www.yes24.com/Product/Goods/44136413">http://www.yes24.com/Product/Goods/44136413</a></p>
</blockquote>
<h2 id="2장-강화학습-기초-1-가치함수와-벨만-방정식"><a href="#2장-강화학습-기초-1-가치함수와-벨만-방정식" class="headerlink" title="2장 강화학습 기초 1: 가치함수와 벨만 방정식"></a>2장 강화학습 기초 1: 가치함수와 벨만 방정식</h2><hr>
<p>이제 에이전트가 학습할 수 있도록 문제를 <code>MDP</code>로 정의 했다. 에이전트는 <code>MDP</code>를 통해 <strong>최적 정책</strong> 을 찾으면 된다. 이제 에이전트가 어떻게 최적 정책을 찾을 수 있을지 보다 구체적으로 알아보도록 하자!<br><strong>(❗️수식 주의)</strong></p>
<h3 id="가치함수"><a href="#가치함수" class="headerlink" title="가치함수"></a>가치함수</h3><p>에이전트의 입장에서 어떤 행동을 하는 것이 좋은지는 어떻게 알 수 있을까? 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할텐데, 아직 받지 않는 보상들을 어떻게 고려한단 말일까?🧐 <strong>앞으로 받을 보상에 관련된 개념</strong> 이 바로 <code>가치 함수</code>이다.</p>
<p>현재 시간 t로부터 에이전트가 행동을 하면서 받을 보상들을 단순히 합한다는 생각을 해볼 수 있다. 그렇게 되면 $R_{t+1}+R_{t+2}+R_{t+3}+R_{t+4}+R_{t+5}+…$와 같은 꼴일 것이다. 그런데 시간에 따른 보상을 이렇게 단순하게 더한다면 세가지 문제가 생긴다.</p>
<ol>
<li><strong>지금 받은 보상이나 미래에 받는 보상이나 똑같이 취급한다.</strong></li>
<li><strong>100이라는 보상을 1번 받는 것과 20이라는 보상을 5번 받는 것을 구분할 방법이 없다.</strong></li>
<li><strong>시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대이고 1씩 받아도 합이 무한대이다.</strong></li>
</ol>
<p>이러한 문제 때문에 에이전트는 단순한 보상의 합으로는 <strong>시간 t에 있었던 상태가 어떤 가치를 가지는지</strong> 판단하기가 어렵다. 따라서 좀 더 정확하게 상태의 가치를 판단하기 위해 이전 포스트에서 설명한 <code>할인율</code>이라는 개념을 사용한다. 할인율을 적용하여 시간 t 이후의 <strong>시간 t시점에서의 보상</strong> 을 모두 더한 것을 <code>반환값</code>이라고 하며 $G_t$로 표현한다. 반환값을 수식으로 표현하면 아래와 같다.</p>
<p>$$G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\gamma^4 R_{t+5}…$$</p>
<p>반환값이라는 것은 에이전트가 실제로 환경을 탐험하며 받은 보상의 합으로, 이 책에서는 에이전트와 환경이 <strong>유한한 시간동안 상호작용</strong> 하는 경우만 다룬다. 이렇게 <strong>유한한 에이전트와 환경의 상호작용</strong> 을 <code>에피소드</code>라고 부른다. 에피소드 에서는 에피소드를 끝낼 수 있는 마지막 상태가 있는데, 체스의 경우를 생각한다면 킹을 잃는 순간이 마지막 상태가 된다.</p>
<p>에이전트가 에피소드가 끝난 후에 <strong>‘그때로부터 얼마의 보상을 받았지?’</strong> 라며 보상을 정산하는 것이 반환값이다. 만일 에피소드를 t=1 부터 5까지 진행했다면 에피소드가 끝난 후에 방문했던 상태들에 대한 5개의 반환값이 생길 것이다.</p>
<p>$$<br>G_1 = R_{2}+\gamma R_{3}+\gamma^2 R_{4}+\gamma^3 R_{5}+\gamma^4 R_{6}\\<br>G_2 = R_{3}+\gamma R_{4}+\gamma^2 R_{5}+\gamma^3 R_{6}\\<br>G_3 = R_{4}+\gamma R_{5}+\gamma^2 R_{6}\\<br>G_4 = R_{5}+\gamma R_{6}\\<br>G_5 = R_{6}<br>$$</p>
<p>MDP로 정의 되는 세계에서 반환값은 에피소드마다 다를 수 있다. 때문에 에이전트는 특정 상태의 가치를 <strong>반환값에 대한 기댓값</strong> 으로 판단해야 한다. 이것이 바로 <code>가치함수</code>의 개념이다. 따라서 가치함수는 아래와 같이 표현된다.</p>
<p>$$v(s) = E[G_t|S_t=s]$$</p>
<p>각 타임스텝마다 받는 보상이 모두 확률적이고 반환값이 그 보상들의 합이므로 반환값은 <strong>확률변수</strong> 이다. 하지만 가치함수는 확률변수가 아니라 특정 양을 나타내는 값이므로 <strong>소문자</strong>로 표현한다. 가치함수의 식에 위에서 정의한 반환값의 수식을 대입하고 정리해보면 아래와 같다.</p>
<p>$$<br>v(s) = E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}…|S_t=s]\\<br> = E[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}…)|S_t=s]\\<br> = E[R_{t+1}+\gamma G_{t+1}|S_t=s]<br>$$</p>
<p>사실 $R_{t+2}+\gamma R_{t+3} …$ 부분을 반환값의 형태로 표현하긴 했지만 사실 에이전트가 실제로 받은 보상이 아니다. 이 보상은 <strong>앞으로 받을 것이라 예상하는 보상</strong> 으로써 이 부분 또한 가치함수로 표현할 수 있다.</p>
<p>$$v(s) = E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]$$</p>
<p>여기까지는 가치함수를 정의할 때 정책을 고려하지 않았다. 하지만 에이전트가 앞으로 받을 보상에 대해 생각할 때 정책을 고려하지 않으면 안된다. 왜냐하면 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야 하고 <strong>각 상태에서의 행동을 결정하는 것</strong> 이 에이전트의 정책이기 때문이다.</p>
<p>보상은 어떤 상태에서 어떤 행동을 하는지에 따라 환경에서 에이전트에게 주어진다. 이말은 곧 <code>MDP</code>로 정의 되는 문제에서의 가치함수가 행동을 결정하는 <code>정책</code>에 의존한다는 것이다. 따라서 아래의 수식처럼 <strong>가치함수에 아래 첨자로 정책을 쓰면</strong> 더 명확한 수식이 된다.</p>
<p>$$v_\pi (s) = E_\pi [R_{t+1}+ \gamma v_\pi (S_{t+1})|S_t = s]$$</p>
<p>이 수식이 바로 강화학습에서 상당히 중요한 <code>벨만 기대 방정식(Bellman Expectation Equation)</code>이다.<br>벨만 기대 방정식은 <strong>현재 상태의 가치함수 $v_\pi (s)$와 다음상태의 가치함수 $v_\pi (S_{t+1})$ 사이의 관계</strong> 를 말해주는 방정식이다.</p>
<blockquote>
<p>강화학습은 벨만 방정식을 어떻게 풀어가느냐의 스토리이다 🤧</p>
</blockquote>
<h3 id="큐함수"><a href="#큐함수" class="headerlink" title="큐함수"></a>큐함수</h3><p>가치함수는 말 그대로 <strong>“함수”</strong> 이다. 따라서 입력이 무엇이고 출력이 무엇인지 알 필요가 있다. 지금까지 설명한 가치함수는 <code>상태 가치함수(state value-function)</code>으로써 <strong>상태</strong> 가 입력으로 들어오면 그 상태에서 <strong>앞으로 받을 보상의 합</strong> 을 출력하는 함수이다. 따라서 에이전트는 가치함수를 통해 어떤 상태에 있는 것이 얼마나 좋은지를 알 수 있다.</p>
<p>상태 가치함수가 각 상태에 대해 가치를 알려주는 것처럼 각 행동에 대해 가치를 알려주는 함수가 있다면 어떨까? 아마 에이전트는 그 함수의 값만 보고 바로 행동을 선택할 수 있을 것이다. <strong>어떤 상태에서 어떤 행동이 얼마나 좋은지</strong> 알려주는 함수를 <code>큐함수(Q Function)</code>, 다른말로 행동 가치함수 라고 한다.</p>
<img src="/2021/01/22/Reinforcement-Learning-Basic-2/qfunction.jpeg" class="" width="200" height="200" title="큐함수" alt="큐함수">

<p>큐함수는 상태, 행동이라는 두가지 변수를 가지며 $q_\pi (s,a)$라고 나타낸다. 또한 가치함수와 큐함수 사이의 관계식은 다음과 같이 표현할 수 있다.</p>
<p>$$v_\pi (s) = \sum_{a \in A} \pi (a|s) q_\pi (s,a)$$</p>
<ol>
<li>각 행동을 했을 때 앞으로 받을 보상인 큐함수 $q_\pi (s,a)$를 정책 $\pi (a|s)$에 곱한다</li>
<li>모든 행동에 대해 큐함수와 $\pi (a|s)$를 곱한 값을 더하면 가치함수가 된다.</li>
</ol>
<p>큐함수는 강화학습에서 중요한 역할을 한다. <strong>강화학습에서 에이전트가 행동을 선택하는 기준</strong> 으로 가치 함수 보다는 보통 <strong>큐함수</strong> 를 사용한다. 그 이유는 뒤에서 등장한다! 큐함수 또한 벨만 기대 방정식의 형태로 나타 낼수 있으며 조건문에 행동이 들어간다는 점에서 가치함수의 식과 다르다.</p>
<p>$$q_\pi (s,a) = E_\pi [R_{t+1}+ \gamma q_\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]$$</p>
<h3 id="벨만-기대-방정식"><a href="#벨만-기대-방정식" class="headerlink" title="벨만 기대 방정식"></a>벨만 기대 방정식</h3><p>이제 2장의 메인 디쉬인 <code>벨만 기대 방정식</code>을 찬찬히 살펴보자(👂🏻집중). 살짝 정리를 해보면 벨만 <strong>기대</strong> 방정식이라고 하는 이유는 식에 <strong>기댓값의 개념</strong> 이 들어가기 때문이고, 이 벨만 방정식은 <strong>현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계</strong> 를 식으로 나타낸 것이었다.</p>
<p>벨만 방정식은 강화학습에서 상당히 중요한 부분을 차지한다. 벨만 방정식이 강화학습에서 왜 그렇게 중요한 위치를 차지하고 있는 걸까?🤔 앞에서 정의했던 가치함수의 정의를 다시 한번 살펴보자.</p>
<p>$$v_\pi (s) = E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}…|S_t=s]$$</p>
<p>이 수식으로 부터 기댓값을 알아내려면 앞으로 받을 보상에 대해 고려해야하고, 이는 정의상으로는 가능하지만 <strong>상태가 많아질수록 상당히 비효율적인 방법</strong> 이다. 따라서 컴퓨터가 이 기댓값을 계산하기 위해 다른 조치가 필요하다.</p>
<p>예를 들어 1을 100번 더해야하는 문제가 있다고 해보자. 식 하나로 풀어내는 방법은 아래와 같다</p>
<p>$$1+1+1+…+1 = 100$$<br>하지만 다른 방법으로 접근해볼 수도 있다. x라는 변수를 지정해 그 값에 1을 계속 더해나가는 것이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">  X = X + <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>벨만 방정식으로 가치함수를 계산하는 것은 두 번째 방식과 같은 것이다. 한 번에 모든 것을 계산하는 것이 아니라 값을 변수에 저장하고, 루프를 도는 계산을 통해 참 값을 알아나가는 것이다. 즉, <strong>현재 가치함수 값을 업데이트</strong> 해 나가는 것이다. 하지만 업데이트 하려면 기댓값을 계산해야 하는데 기댓값은 어떻게 계산할 수 있을까?</p>
<p>기댓값에는 어떠한 행동을 할 확률(<strong>정책 $\pi (a|s)$</strong>)과 그 행동을 했을 때 어떤 상태로 가게 되는 확률(<strong>상태 변환 확률 $P_{ss’}^a$</strong>)이 포함되어 있다. 따라서 정책과 상태 변환 확률을 포함해서 계산하면 된다.</p>
<p>$$v_\pi (s) = \sum_{a \in A} \pi (a|s) (r_{(s,a)} + \gamma \sum_{s’ \in S} P_{ss’}^a v_\pi (s’) )$$</p>
<p>예시를 한번 살펴보는 것이 매우 도움이 될 것이다.</p>
<img src="/2021/01/22/Reinforcement-Learning-Basic-2/bellman.png" class="" width="500" height="200" title="벨만 기대 방정식의 예시" alt="벨만 기대 방정식의 예시">
<p>설명을 용이하게 하기 위해,</p>
<ol>
<li><code>상태 변환 확률</code>($P_{ss’}^a$)은 모두 1이라고 생각하고(왼쪽으로 가기로 결정했다면 1의 확률로 왼쪽으로 간다고 하자)</li>
<li><code>정책</code>($\pi (a|s)$)은 무작위로 행동하는 것으로서 각 행동이 25%의 확률로 선택이 되며,</li>
<li><code>할인율</code>($\gamma$)은 0.9라고 생각하고,</li>
<li>$s’$은 모든 상태일 수 있으나, 에이전트의 행동으로 도착하게 될 상태라고 해보자.</li>
<li>회색 별은 $r(s,a)$에 해당하는 보상 값이다.</li>
</ol>
<p>위의 가정을 적용하여 식을 바꾸어 본다면,</p>
<p>$$ v<em>\pi (s) = \sum</em>{a \in A} 0.25 (r*{(s,a)} + (0.9)v*\pi (s’) ) $$</p>
<p>와 같이 바꿀 수 있고, 위의 표와 같이 계산을 할 수 있게 된다.</p>
<blockquote>
<p>벨만 기대 방정식을 이용해 현재의 가치함수를 계속 업데이트 하다 보면,<br>에이전트가 얻을 실제 보상에 대한 참 기댓값을 얻을 수 있다.</p>
</blockquote>
<h3 id="벨만-최적-방정식"><a href="#벨만-최적-방정식" class="headerlink" title="벨만 최적 방정식"></a>벨만 최적 방정식</h3><p>처음의 가치함수의 값들은 의미가 없는 값으로 초기화 된다. 초기값으로 부터 시작해서 <code>벨만 기대 방정식</code>으로 반복적으로 계산한다고 가정해보자. 이 계산을 반복하다보면 방정식의 우항과 좌항이 같아진다(무한히 반복한다는 가정하에). 즉, $v_\pi (s)$값이 수렴하는 것이다. 그렇다면 현재의 정책 $\pi$에 대한 <strong>참 가치함수</strong> 를 구한것이다.</p>
<p>$$v_{k+1}(s) \leftarrow \sum_{a \in A} \pi (a|s)(r_{(s,a)}+ \gamma v_k(s’))$$</p>
<p>벨만 기대 방정식을 기댓값을 계산하기 위해 살짝 변형하면, 현재 정책에 대한 <strong>참 가치함수</strong> 를 구할 수 있다. 위의 식은 뒤에서 배울 dynamic programming에서 자세히 다루도록 한다.</p>
<p>그러나 <strong>참 가치함수</strong> 와 <strong>최적 가치함수(Optimal Value Function)</strong> 은 다르다⭐️. <code>참 가치함수</code>는 <strong>어떤 정책을 따라서 움직였을 경우에 받게되는 보상에 대한 참값</strong> 이고, <code>최적의 가치함수</code>는 <strong>수많은 정책 중에서 가장 높은 보상을 얻게 되는 정책을 따랐을 때의 가치함수</strong> 이다.</p>
<p>그렇다. 우리는 단순히 현재 정책에 대한 가치함수를 구하고 싶은게 아닌, <strong>최적 정책</strong>을 찾고 싶은 것이다. 단순히 현 정책에 대한 가치함수를 찾는 것이 아닌 더 좋은 정책으로 현재의 정책을 업데이트 해 나가야 한다. 이쯤에서 이런 질문이 들어야 한다.</p>
<ul>
<li>더 좋은 정책이라는 것의 정의는 무엇인가?</li>
<li>어떤 정책이 더 좋은 정책이라고 판단할 수 있을까?</li>
</ul>
<p>더 좋은 정책이란 정책을 따라갔을 때 받을 보상들의 합이 더 큰 경우라고 말할 수 있을 것이다. 그리고 그것은 <strong>가치함수</strong> 를 통해 판단할 수 있었다. 결국 가치함수가 정책이 얼마나 좋은지를 말해주는 것이다. 따라서 <strong>모든 정책 중, 가장 큰 가치함수를 갖는 정책이 최적 정책이다.</strong> 최적 큐함수 또한 같은 방식으로 생각한다.</p>
<p>$$<br>v^*(s) = max_{\pi}[v_\pi (s)]\\<br>q^*(s,a) = max_{\pi}[q_\pi (s,a)]<br>$$</p>
<p>가장 높은 가치함수(큐함수)를 에이전트가 찾았다고 가정해보자. 이때 <strong>최적 정책</strong> 은 각 상태 s에서의 최적의 큐함수에 대해 가장 큰 값을 가진 행동을 하는 것이다. 즉, 선택 상황에서 판단 기준은 큐함수이며, 최적 정책은 최적 큐함수 $q^*$만 안다면 아래와 같이 구할 수 있다.</p>
<p>$$<br>\pi^*(s,a) = \begin{cases}<br>  \displaystyle 1, \;if \; a = argmax_{a \in A} \;q^*(s,a) \\<br>  \displaystyle 0, \;otherwise<br>\end{cases}<br>$$</p>
<p>그렇다면 최적의 큐함수는 어떻게 구할 수 있을까?<br>그것을 구하는 것이 <strong>순차적 행동 결정 문제(MDP)</strong> 를 푸는 것이다. 어떻게 최적의 가치함수를 구하는지에 대해서는 다음 장에서 다룬다. 여기서는 최적의 가치함수 끼리 관계가 어떻게 되는지를 살펴보자.</p>
<p>현재 상태의 가치함수가 최적이라고 가정해보자. 현재상태의 가치함수가 최적이라는 것은 에이전트가 가장 좋은 행동을 선택한다는 것이다. 이때 선택의 기준이 되는 큐함수는 최적의 큐함수 이어야 하고, 따라서 다음이 최적의 가치함수의 식이 된다.<br>$$v^*(s) = max_{a}[q^*(s,a) | S_t=s, A_t=a]$$</p>
<p>여기서 큐함수를 가치함수로 고쳐서 표현하면 아래와 같다.</p>
<p>$$v^*(s) = max_{a}E[R_{t+1} + \gamma v^*(S_{t+1}) | S_t=s, A_t=a]$$</p>
<p>바로 이 식을 <code>벨만 최적 방정식(Bellman Optimality Equation)</code> 이라 부르며, 이식은 최적의 가치함수에 대한 것이다. 큐함수에 대해서도 벨만 최적 방정식을 표현할 수 있는데, 아래와 같이 표현한다.</p>
<p>$$q^*(s,a) = E[R_{t+1} + \gamma max_{a’}q^*(S_{t+1}, a’) | S_t=s, A_t=a]$$</p>
<p>기댓값인 이유는 다음 상태가 상태 변환 확률에 따라 달라지기 때문이다. 벨만 기대 방정식과 벨만 최적 방정식을 이용해 MDP로 정의되는 문제를 <strong>계산</strong> 으로 푸는 방법이 바로 다음장에서 다룰 <code>다이내믹 프로그래밍(Dynamic programming)</code>이다.</p>
<h3 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h3><ol>
<li><code>MDP</code>: 순차적 행동 결정 문제를 수학적으로 정의하는 것. 상태($S_t$), 행동($A_t$), 보상함수($r(s,a)$), 상태 변환 확률($P_{ss’}^a$), 할인율($\gamma$)로 구성.</li>
<li><code>가치함수</code>: 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합.<br>$$v_\pi (s) = E_\pi [R_{t+1}+ \gamma v_\pi (S_{t+1})|S_t = s]$$</li>
<li><code>큐함수</code>: 각 행동에 대해 가치를 알려주는 함수, 정책 업데이트 시에 사용<br>$$q_\pi (s,a) = E_\pi [R_{t+1}+ \gamma q_\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]$$</li>
<li><code>벨만 기대 방정식</code>: 현재 상태의 가치함수와 다음 상태 가치함수의 관계식<br>$$v_\pi (s) = E_\pi [R_{t+1}+ \gamma v_\pi (S_{t+1})|S_t = s]$$</li>
<li><code>벨만 최적 방정식</code>: 최적의 정책을 따르는 가치함수와 다음 상태 가치함수의 관계식<br>$$v^*(s) = max_{a}E[R_{t+1} + \gamma v^*(S_{t+1}) | S_t=s, A_t=a]$$</li>
</ol>
<h3 id="2장-한줄평"><a href="#2장-한줄평" class="headerlink" title="2장 한줄평"></a>2장 한줄평</h3><blockquote>
<p>최적 방정식 부분을 좀 더 명확하게 다시 정리해 봐야겠다. 🤥</p>
</blockquote>

        </div>
        <footer class="article-footer">
            



    <a data-url="https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/" data-id="clck73wiz0007sabrce4t6w5s" class="article-share-link"><i class="fa fa-share"></i>Share</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "Jangyeong Kim"
        },
        "headline": "강화학습 기초: 가치함수와 벨만방정식",
        "image": "https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/qfunction.jpeg",
        "keywords": "ReinforcementLearning BellmanEquation ValueFunction",
        "genre": "Tech 강화학습",
        "datePublished": "2021-01-22",
        "dateCreated": "2021-01-22",
        "dateModified": "2023-01-06",
        "url": "https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/",
        "description": "
본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다.http://www.yes24.com/Product/Goods/44136413

2장 강화학습 기초 1: 가치함수와 벨만 방정식
이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의 했다. 에이전트는 MDP를 통해 최적 정책 을 찾으면 된다. 이제 에이전트가 어떻게 최적 ",
        "wordCount": 1593
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>


    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="linkedin" href="https://www.linkedin.com/in/jangyeong-kim-b7924422a/" target="_blank" rel="noopener">
                        <i class="icon fa fa-linkedin"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/jangyoung0108/" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="instagram" href="https://www.instagram.com/long_shiine/" target="_blank" rel="noopener">
                        <i class="icon fa fa-instagram"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/longshiine" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2021/01/23/Reinforcement-Learning-Basic-3/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            강화학습 기초: 그리드월드와 다이내믹 프로그래밍
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2021/01/20/Reinforcement-Learning-Basic-1/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">강화학습 기초: MDP(Markov Decision Process)</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/12/31/2022-Retrospect/" class="thumbnail">
    
    
        <span style="background-image:url(https://images.unsplash.com/photo-1641945512297-538a81b13f85?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1770&q=80)" alt="2022년 회고, 스타트업 실패에 대하여" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Writing/">Writing</a></p>
                            <p class="item-title"><a href="/2022/12/31/2022-Retrospect/" class="title">2022년 회고, 스타트업 실패에 대하여</a></p>
                            <p class="item-date"><time datetime="2022-12-31T09:32:06.000Z" itemprop="datePublished">Dec 31, 2022</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2022/12/28/Himalaya-Fantasy-1/" class="thumbnail">
    
    
        <span style="background-image:url(https://images.unsplash.com/photo-1584884255471-e92d057f10ed?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1770&q=80)" alt="&lt;히말라야 환상방황&gt; - EP1. 혼돈의 타멜거리" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Writing/">Writing</a></p>
                            <p class="item-title"><a href="/2022/12/28/Himalaya-Fantasy-1/" class="title">&lt;히말라야 환상방황&gt; - EP1. 혼돈의 타멜거리</a></p>
                            <p class="item-date"><time datetime="2022-12-28T09:14:19.000Z" itemprop="datePublished">Dec 28, 2022</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2021/07/19/StarGAN-v2-Review/" class="thumbnail">
    
    
        <span style="background-image:url(/2021/07/19/StarGAN-v2-Review/intro.png)" alt="StarGAN-v2 논문 리뷰" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Tech/">Tech</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></p>
                            <p class="item-title"><a href="/2021/07/19/StarGAN-v2-Review/" class="title">StarGAN-v2 논문 리뷰</a></p>
                            <p class="item-date"><time datetime="2021-07-19T05:26:32.000Z" itemprop="datePublished">Jul 19, 2021</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2021/02/05/Priority-Queue/" class="thumbnail">
    
    
        <span style="background-image:url(/2021/02/05/Priority-Queue/write2.jpeg)" alt="Priority Queue(우선순위큐)" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Tech/">Tech</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/">알고리즘</a></p>
                            <p class="item-title"><a href="/2021/02/05/Priority-Queue/" class="title">Priority Queue(우선순위큐)</a></p>
                            <p class="item-date"><time datetime="2021-02-05T12:46:32.000Z" itemprop="datePublished">Feb 05, 2021</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2021/02/05/DFS-BFS/" class="thumbnail">
    
    
        <span style="background-image:url(/2021/02/05/DFS-BFS/write.jpeg)" alt="DFS와 BFS" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Tech/">Tech</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/">알고리즘</a></p>
                            <p class="item-title"><a href="/2021/02/05/DFS-BFS/" class="title">DFS와 BFS</a></p>
                            <p class="item-date"><time datetime="2021-02-05T04:50:29.000Z" itemprop="datePublished">Feb 05, 2021</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Tech/">Tech</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Tech/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/">강화학습</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tech/%EA%B3%B5%ED%95%99%EC%88%98%ED%95%99/">공학수학</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tech/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tech/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/">알고리즘</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Writing/">Writing</a><span class="category-list-count">3</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 10px;">BFS</a> <a href="/tags/BellmanEquation/" style="font-size: 10px;">BellmanEquation</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/ComputerVision/" style="font-size: 10px;">ComputerVision</a> <a href="/tags/DFS/" style="font-size: 10px;">DFS</a> <a href="/tags/DeepLearning/" style="font-size: 16.67px;">DeepLearning</a> <a href="/tags/Dot-Product/" style="font-size: 10px;">Dot Product</a> <a href="/tags/DynamicProgramming/" style="font-size: 10px;">DynamicProgramming</a> <a href="/tags/GAN/" style="font-size: 10px;">GAN</a> <a href="/tags/GradientVanishing/" style="font-size: 10px;">GradientVanishing</a> <a href="/tags/GridWorld/" style="font-size: 10px;">GridWorld</a> <a href="/tags/Inner-Product/" style="font-size: 10px;">Inner Product</a> <a href="/tags/MDP/" style="font-size: 10px;">MDP</a> <a href="/tags/MonteCarloApproximation/" style="font-size: 10px;">MonteCarloApproximation</a> <a href="/tags/Nepal/" style="font-size: 10px;">Nepal</a> <a href="/tags/OptimalAuction/" style="font-size: 10px;">OptimalAuction</a> <a href="/tags/PolicyIteration/" style="font-size: 10px;">PolicyIteration</a> <a href="/tags/PriorityQueue/" style="font-size: 10px;">PriorityQueue</a> <a href="/tags/QLearning/" style="font-size: 10px;">QLearning</a> <a href="/tags/ReinforcementLearning/" style="font-size: 20px;">ReinforcementLearning</a> <a href="/tags/SARSA/" style="font-size: 10px;">SARSA</a> <a href="/tags/TemporalDifference/" style="font-size: 10px;">TemporalDifference</a> <a href="/tags/Travel/" style="font-size: 10px;">Travel</a> <a href="/tags/ValueFunction/" style="font-size: 10px;">ValueFunction</a> <a href="/tags/ValueIteration/" style="font-size: 10px;">ValueIteration</a> <a href="/tags/Vector/" style="font-size: 10px;">Vector</a> <a href="/tags/heapq/" style="font-size: 10px;">heapq</a> <a href="/tags/%EC%8A%A4%ED%83%80%ED%8A%B8%EC%97%85/" style="font-size: 10px;">스타트업</a> <a href="/tags/%EC%9D%BC%EA%B8%B0/" style="font-size: 10px;">일기</a> <a href="/tags/%ED%9A%8C%EA%B3%A0/" style="font-size: 10px;">회고</a>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2023 Jangyeong Kim</p>
                
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

    </div>
    
    
    <script>
    var disqus_shortname = 'longshiine';
    
    
    var disqus_url = 'https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>





    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>

    

    
      <script data-ad-client="ca-pub-2555870048154123" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

    
    
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


</body>
</html>
